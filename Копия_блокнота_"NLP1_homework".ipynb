{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geriys/VKirillV/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22NLP1_homework%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ML1_1:\n",
        "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true\n",
        "\n",
        "###ML1_2:\n",
        "https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true\n",
        "\n",
        "###ML1_3:\n",
        "https://www.hackerrank.com/challenges/detect-html-links/problem?isFullScreen=true\n",
        "\n",
        "###ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
      ],
      "metadata": {
        "id": "dH7qx_irU4Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML1_1:"
      ],
      "metadata": {
        "id": "xJfkstKpqsXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Regex_Pattern = r'(ok){3,}'\t# Do not delete 'r'."
      ],
      "metadata": {
        "id": "mrMjeykGyYME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML1_2:"
      ],
      "metadata": {
        "id": "tdUp3v4hyaG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Regex_Pattern = ^\\d{2}(-(?:--)?|\\.|:)\\d{2}\\1\\d{2}\\1\\d{2}$"
      ],
      "metadata": {
        "id": "HVZBbNUgbiRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Regex_Pattern = '^(\\d{2}(?:-|:|---|\\.){1}){3}\\d{2}$'"
      ],
      "metadata": {
        "id": "yvkq_ZDVfQKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML1_3:"
      ],
      "metadata": {
        "id": "pO82NMbIbiWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "n = int(input())\n",
        "l = []\n",
        "for i in range(n):\n",
        "    x = str(input())\n",
        "    l.append(x)\n",
        "pattern = re.compile('<a href=\"(.+?)\".*?>(.*?)<\\/a>')\n",
        "for x in l:\n",
        "        a = re.findall(pattern, x)\n",
        "        for z in a:\n",
        "            y1 = z[0].strip()\n",
        "            y2 = z[1].strip()\n",
        "            p2 = re.compile('[<.*>]*(.*?)<.*>')\n",
        "            y2 = re.findall(p2, y2)\n",
        "            if(len(y2) != 0):\n",
        "                y2 = str(y2)\n",
        "                y2 = y2[2:len(y2)-2]\n",
        "                zz = y2.find('>')\n",
        "                y2 = y2[zz+1 : len(y2)]\n",
        "                print(y1+','+y2)\n",
        "            else:\n",
        "                print(y1 + ',' + z[1].strip())"
      ],
      "metadata": {
        "id": "sSl3yguUbiYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
      ],
      "metadata": {
        "id": "9COcYFvhbidC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk sklearn"
      ],
      "metadata": {
        "id": "R43n3IqaxBZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXDzb56xTvSi",
        "outputId": "475e1ec4-be0d-46bb-b496-eed265a0661c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m51.2/55.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=04cbb3a929330f5250a09f24facaf8ba2ca4298bc1d5f98b550239070e19e718\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "pIlp72ECm6mP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-U5XAZhxBck",
        "outputId": "e40b87d2-c2f6-4416-e1c0-ae726b8e41af"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "bZLYpDeKxBet"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из CSV файла\n",
        "df = pd.read_csv('labeled.csv')\n",
        "\n",
        "# Инициализация стеммера и лемматизатора\n",
        "# stemmer = PorterStemmer()\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Инициализация морфологического анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Функция для стемминга\n",
        "def stem_sentences(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# # Функция для лемматизации\n",
        "# def lemmatize_sentences(sentence):\n",
        "#     tokens = word_tokenize(sentence)\n",
        "#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "#     return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize_sentences(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    lemmatized_tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Применение стемминга и лемматизации к комментариям\n",
        "df['stemmed_comments'] = df['comment'].apply(stem_sentences)\n",
        "df['lemmatized_comments'] = df['comment'].apply(lemmatize_sentences)\n",
        "\n",
        "# Создание матрицы признаков BoW\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(df['comment'])\n",
        "\n",
        "# Вывод результатов\n",
        "print(df[['comment', 'stemmed_comments', 'lemmatized_comments']].head())\n",
        "# print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GPjCe9UxBgv",
        "outputId": "b7847203-fa3b-400c-b0a7-3dceae43428e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comment  \\\n",
            "0               Верблюдов-то за что? Дебилы, бл...\\n   \n",
            "1  Хохлы, это отдушина затюканого россиянина, мол...   \n",
            "2                          Собаке - собачья смерть\\n   \n",
            "3  Страницу обнови, дебил. Это тоже не оскорблени...   \n",
            "4  тебя не убедил 6-страничный пдф в том, что Скр...   \n",
            "\n",
            "                                    stemmed_comments  \\\n",
            "0                верблюдов-т за что ? дебил , бл ...   \n",
            "1  хохл , эт отдушин затюкан россиянин , мол , во...   \n",
            "2                                собак - собач смерт   \n",
            "3  страниц обнов , деб . эт тож не оскорблен , а ...   \n",
            "4  теб не убед 6-страничн пдф в том , что скрипал...   \n",
            "\n",
            "                                 lemmatized_comments  \n",
            "0                 верблюд-то за что ? дебил , бл ...  \n",
            "1  хохол , это отдушина затюканый россиянин , мол...  \n",
            "2                            собака - собачий смерть  \n",
            "3  страница обновить , дебил . это тоже не оскорб...  \n",
            "4  ты не убедить 6-страничный пдф в тот , что скр...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "UIEjnLEoFFkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e2f4e6-eaee-4719-9c5c-d024b8055f6e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['comment', 'toxic', 'stemmed_comments', 'lemmatized_comments'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gn6370oiC3p1",
        "outputId": "59eb9c52-2ec9-4b40-ba4b-bb2b82f0523c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             comment  toxic  \\\n",
              "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0   \n",
              "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0   \n",
              "2                          Собаке - собачья смерть\\n    1.0   \n",
              "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0   \n",
              "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0   \n",
              "\n",
              "                                    stemmed_comments  \\\n",
              "0                верблюдов-т за что ? дебил , бл ...   \n",
              "1  хохл , эт отдушин затюкан россиянин , мол , во...   \n",
              "2                                собак - собач смерт   \n",
              "3  страниц обнов , деб . эт тож не оскорблен , а ...   \n",
              "4  теб не убед 6-страничн пдф в том , что скрипал...   \n",
              "\n",
              "                                 lemmatized_comments  \n",
              "0                 верблюд-то за что ? дебил , бл ...  \n",
              "1  хохол , это отдушина затюканый россиянин , мол...  \n",
              "2                            собака - собачий смерть  \n",
              "3  страница обновить , дебил . это тоже не оскорб...  \n",
              "4  ты не убедить 6-страничный пдф в тот , что скр...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99a41325-695a-445e-a9c6-a697bead6110\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>toxic</th>\n",
              "      <th>stemmed_comments</th>\n",
              "      <th>lemmatized_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
              "      <td>1.0</td>\n",
              "      <td>верблюдов-т за что ? дебил , бл ...</td>\n",
              "      <td>верблюд-то за что ? дебил , бл ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>хохл , эт отдушин затюкан россиянин , мол , во...</td>\n",
              "      <td>хохол , это отдушина затюканый россиянин , мол...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Собаке - собачья смерть\\n</td>\n",
              "      <td>1.0</td>\n",
              "      <td>собак - собач смерт</td>\n",
              "      <td>собака - собачий смерть</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>страниц обнов , деб . эт тож не оскорблен , а ...</td>\n",
              "      <td>страница обновить , дебил . это тоже не оскорб...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>теб не убед 6-страничн пдф в том , что скрипал...</td>\n",
              "      <td>ты не убедить 6-страничный пдф в тот , что скр...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99a41325-695a-445e-a9c6-a697bead6110')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99a41325-695a-445e-a9c6-a697bead6110 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99a41325-695a-445e-a9c6-a697bead6110');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-82acf85a-3515-49c7-aaa1-6cbb9b725429\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-82acf85a-3515-49c7-aaa1-6cbb9b725429')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-82acf85a-3515-49c7-aaa1-6cbb9b725429 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 14412,\n  \"fields\": [\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14412,\n        \"samples\": [\n          \"\\u0431\\u0435\\u0437\\u0440\\u043e\\u0434\\u043d\\u044b\\u0439 \\u043f\\u043e\\u0442\\u043e\\u043c\\u043e\\u043a \\u0445\\u043e\\u043b\\u043e\\u043f\\u0430 \\u0440\\u0430\\u0441\\u0441\\u0443\\u0436\\u0434\\u0430\\u0435\\u0442 \\u043e \\u043d\\u0430\\u0446\\u0438\\u043e\\u043d\\u0430\\u043b\\u044c\\u043d\\u043e\\u0439 \\u0433\\u043e\\u0440\\u0434\\u043e\\u0441\\u0442\\u0438.\\n\",\n          \"\\u0418\\u043d\\u0442\\u0435\\u0440\\u0435\\u0441\\u043d\\u0430\\u044f \\u0442\\u0435\\u043c\\u0430, \\u043e\\u0434\\u043d\\u0430\\u043a\\u043e. \\u041e\\u0422\\u041f \\u0432\\u0440\\u043e\\u0434\\u0435 \\u0432\\u0435\\u043d\\u0433\\u0435\\u0440\\u0441\\u043a\\u0438\\u0439 \\u0431\\u0430\\u043d\\u043a, \\u0432 \\u0412\\u0435\\u043d\\u0433\\u0440\\u0438\\u0438 \\u043e\\u043d \\u0441\\u0430\\u043c\\u044b\\u0439 \\u043f\\u043e\\u043f\\u0443\\u043b\\u044f\\u0440\\u043d\\u044b\\u0439, \\u0443 \\u043c\\u0435\\u043d\\u044f \\u0443 \\u0441\\u0430\\u043c\\u043e\\u0433\\u043e \\u0435\\u0433\\u043e \\u0441\\u0447\\u0451\\u0442 \\u0438 \\u043a\\u0430\\u0440\\u0442\\u0430, \\u0438\\u0431\\u043e \\u0443 \\u043d\\u0435\\u0433\\u043e \\u0434\\u043e\\u0433\\u043e\\u0432\\u043e\\u0440 \\u0441 \\u0443\\u043d\\u0438\\u0432\\u0435\\u0440\\u043e\\u043c, \\u0441\\u043a\\u0438\\u0434\\u043a\\u0438-\\u043f\\u043b\\u044e\\u0448\\u043a\\u0438-\\u0432\\u043e\\u0437\\u0432\\u0440\\u0430\\u0442 \\u0438 \\u0442.\\u043f. \\u0434\\u043b\\u044f \\u0441\\u0442\\u0443\\u0434\\u0435\\u043d\\u0442\\u043e\\u0432. \\u0418 \\u0437\\u0430 4 \\u0433\\u043e\\u0434\\u0430 \\u043f\\u043e\\u043b\\u044c\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f \\u043d\\u0438 \\u043e\\u0434\\u043d\\u043e\\u0439 \\u043f\\u0440\\u043e\\u0431\\u043b\\u0435\\u043c\\u044b, \\u043d\\u0438 \\u043e\\u0434\\u043d\\u043e\\u0439 \\u043f\\u043e\\u0434\\u043a\\u043b\\u044e\\u0447\\u0451\\u043d\\u043d\\u043e\\u0439 \\u0443\\u0441\\u043b\\u0443\\u0433\\u0438, \\u0431\\u043b\\u043e\\u043a\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f, \\u0441\\u043f\\u0438\\u0441\\u0430\\u043d\\u0438\\u044f \\u043d\\u0438 \\u0437\\u0430 \\u0447\\u0442\\u043e \\u0438 \\u043f\\u043e\\u0434\\u043e\\u0431\\u043d\\u043e\\u0439 \\u0435\\u0440\\u0435\\u0441\\u0438, \\u043a\\u043e\\u0442\\u043e\\u0440\\u043e\\u0439 \\u0441\\u0442\\u0440\\u0430\\u0434\\u0430\\u044e\\u0442 \\u0432\\u0441\\u0435 \\u0440\\u043e\\u0441\\u0441\\u0438\\u0439\\u0441\\u043a\\u0438\\u0435 \\u0431\\u0430\\u043d\\u043a\\u0438. \\u041d\\u0438\\u043a\\u0430\\u043a\\u0438\\u0445 \\u043a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043d\\u044b\\u0445 \\u043a\\u0430\\u0440\\u0442 \\u0434\\u0430\\u0436\\u0435 \\u043d\\u0435 \\u043f\\u0440\\u0435\\u0434\\u043b\\u0430\\u0433\\u0430\\u044e\\u0442 (\\u0438 \\u0432\\u043e\\u043e\\u0431\\u0449\\u0435 \\u043e \\u043f\\u043e\\u0434\\u043e\\u0431\\u043d\\u044b\\u0445 \\u0438\\u0441\\u0442\\u043e\\u0440\\u0438\\u044f\\u0445 \\u043d\\u0435 \\u0441\\u043b\\u044b\\u0448\\u0430\\u043b). \\u0412\\u044b\\u0445\\u043e\\u0434\\u0438\\u0442, \\u0431\\u0430\\u043d\\u043a \\u0442\\u043e\\u0442 \\u0436\\u0435, \\u0430 \\u043f\\u0440\\u0438\\u043d\\u0446\\u0438\\u043f \\u0440\\u0430\\u0431\\u043e\\u0442\\u044b \\u0434\\u0440\\u0443\\u0433\\u043e\\u0439, \\u0437\\u0430\\u0442\\u043e\\u0447\\u0435\\u043d\\u043d\\u044b\\u0439 \\u043f\\u043e\\u0434 \\u0440\\u043e\\u0441\\u0441\\u0438\\u0439\\u0441\\u043a\\u0438\\u0439 \\u043a\\u043b\\u0438\\u0435\\u043d\\u0442 - \\u043b\\u043e\\u0445 . P.S. \\u0412 \\u0412\\u0435\\u043d\\u0433\\u0440\\u0438\\u0438 \\u0432\\u0441\\u0435 \\u0431\\u0430\\u043d\\u043a\\u0438 \\u0448\\u043b\\u044e\\u0442 \\u043a\\u0430\\u0440\\u0442\\u044b \\u043f\\u043e \\u043f\\u043e\\u0447\\u0442\\u0435, \\u043c\\u043e\\u044f \\u043a\\u0430\\u043a \\u0440\\u0430\\u0437 \\u0432 \\u044d\\u0442\\u043e\\u043c \\u043c\\u0435\\u0441\\u044f\\u0446\\u0435 \\u043f\\u0440\\u0438\\u0448\\u043b\\u0430. \\u041d\\u043e \\u0432 \\u043f\\u043e\\u0447\\u0442\\u043e\\u0432\\u044b\\u0439 \\u044f\\u0449\\u0438\\u043a \\u0438\\u0445 \\u043d\\u0438 \\u0437\\u0430 \\u0447\\u0442\\u043e \\u043d\\u0435 \\u043a\\u0438\\u043d\\u0443\\u0442. \\u041b\\u0438\\u0431\\u043e \\u0432 \\u0440\\u0443\\u043a\\u0438, \\u043b\\u0438\\u0431\\u043e \\u0438\\u0437\\u0432\\u0435\\u0449\\u0435\\u043d\\u0438\\u0435 \\u0447\\u0442\\u043e\\u0431 \\u0437\\u0430\\u0431\\u0440\\u0430\\u043b \\u043d\\u0430 \\u043f\\u043e\\u0447\\u0442\\u0435. \\u0410 \\u043f\\u043e\\u0447\\u0442\\u0430 \\u043f\\u043e\\u0440\\u044f\\u0434\\u043e\\u0447\\u043d\\u0430\\u044f\\n\",\n          \"\\u0421\\u0443\\u0442\\u044c \\u0442\\u0440\\u0435\\u0431\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f \\u043f\\u0440\\u043e \\u041230 - \\u043d\\u0435 \\u043f\\u0440\\u043e\\u0447\\u043d\\u043e\\u0441\\u0442\\u044c, \\u0430 \\u0432\\u043e\\u0434\\u043e\\u043d\\u0435\\u043f\\u0440\\u043e\\u043d\\u0438\\u0446\\u0430\\u0435\\u043c\\u043e\\u0441\\u0442\\u044c. \\u0412\\u043e\\u0434\\u043e\\u043d\\u0435\\u043f\\u0440\\u043e\\u043d\\u0438\\u0446\\u0430\\u0435\\u043c\\u043e\\u0441\\u0442\\u044c \\u0432\\u043e\\u043e\\u0431\\u0449\\u0435 \\u0442\\u043e \\u043e\\u0431\\u043e\\u0437\\u043d\\u0430\\u0447\\u0430\\u0435\\u0442\\u0441\\u044f W, \\u043d\\u0443 \\u0434\\u0430 \\u043d\\u0435 \\u0441\\u0443\\u0442\\u044c, \\u0437\\u0430\\u0447\\u0435\\u043c \\u043c\\u043d\\u0435 \\u043e\\u043d\\u0430 \\u0432 \\u043b\\u0435\\u043d\\u0442\\u0435 \\u043f\\u043e\\u0434 \\u0437\\u0435\\u043c\\u043b\\u0451\\u0439 ?\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"toxic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.47195781877088117,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stemmed_comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14155,\n        \"samples\": [\n          \"\\u043a\\u043e\\u0433\\u0434 \\u0448\\u0430\\u0440\\u0430\\u0445\\u043d\\u0443\\u043b \\u0438\\u0445 6 \\u0432 \\u0433\\u043e\\u0434 , \\u0438\\u0437 \\u043d\\u0438\\u0445 4 \\u0437\\u0430 \\u043c\\u0435\\u0441\\u044f\\u0446-\\u0432\\u043e\\u043e\\u0431\\u0449 \\u0441\\u0442\\u0430\\u043b \\u043e\\u0431\\u044b\\u0447\\u043d \\u0434\\u0435\\u043b \\u0442\\u043e\\u0436 )\",\n          \"\\u043b\\u0435\\u0442 7 \\u043d\\u0430\\u0437\\u0430\\u0434 \\u0436\\u0438\\u043b \\u0432 \\u0434\\u043e\\u043c \\u0441 \\u043e\\u0434\\u043d \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434 . \\u0437\\u0432\\u043e\\u043d \\u0432 \\u0441\\u043a\\u043e\\u0440 \\u043f\\u043e \\u0442\\u0440\\u0430\\u0432\\u043c \\u0433\\u043e\\u043b\\u043e\\u0432 \\u0443 \\u0431\\u043b\\u0438\\u0437\\u043a \\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a \\u0438 \\u043e\\u043f\\u0435\\u0440\\u0430\\u0442\\u043e\\u0440 \\u043e\\u043f\\u0440\\u043e\\u0441 \\u043e\\u0431 \\u0432\\u0441\\u0435\\u043c \\u0434\\u043e\\u0445\\u043e\\u0434 \\u0434\\u043e \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441 \\u043d\\u043e\\u043c\\u0435\\u0440 \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434 , \\u0433\\u043e\\u0432\\u043e\\u0440 , \\u0447\\u0442\\u043e \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434 \\u0432\\u0441\\u0435\\u0433 1 \\u0438 \\u043d\\u0430\\u0434 \\u043d\\u0438\\u043c \\u0434\\u0430\\u0436 \\u043d\\u043e\\u043c\\u0435\\u0440 \\u043d\\u0438\\u043a\\u043e\\u0433\\u0434 \\u043d\\u0435 \\u0432\\u0438\\u0441\\u0435\\u043b . \\u0445\\u043e\\u043b\\u043e\\u0434\\u043d \\u0442\\u043e\\u043d \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440 \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441 . \\u044f \\u0431\\u044b\\u0441\\u0442\\u0440 \\u043f\\u043e\\u043f\\u044b\\u0442\\u0430 \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440 \\u0442\\u043e\\u0442 \\u0436\\u0435 \\u043e\\u0442\\u0432\\u0435\\u0442 , \\u0442\\u0430\\u043a \\u043a\\u0430\\u043a \\u0431\\u043e\\u044f \\u043e\\u0448\\u0438\\u0431 \\u0441 23 \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434 , \\u0432\\u0434\\u0440\\u0443\\u0433 \\u0436\\u0435 \\u0441\\u043a\\u043e\\u0440 \\u043a \\u043d\\u0435\\u043c \\u043f\\u043e\\u0434\\u044a\\u0435\\u0434\\u0435\\u0442 \\u0438 \\u0435 \\u0431\\u0443\\u0434\\u0435\\u0442 \\u0434\\u0430\\u043b\\u0435\\u043a \\u043f\\u043e\\u0442 \\u0438\\u0434\\u0442 \\u0434\\u043e \\u043c\\u043e \\u0431\\u0435\\u0437 \\u043d\\u043e\\u043c\\u0435\\u0440 . 4 \\u043f\\u043e\\u043f\\u044b\\u0442\\u043a \\u0438 \\u044f \\u0431\\u0440\\u043e\\u0441 \\u0442\\u0440\\u0443\\u0431\\u043a , \\u0432\\u044b\\u0432\\u0435\\u043b \\u0442\\u0440\\u0430\\u0432\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d \\u043d\\u0430 \\u0443\\u043b\\u0438\\u0446 \\u0438 \\u0441\\u0440\\u0430\\u0437 \\u044d\\u0442 \\u043a\\u0430\\u0440\\u0442\\u0438\\u043d \\u0437\\u0430\\u043c\\u0435\\u0442 \\u0441\\u043e\\u0441\\u0435\\u0434 \\u0441 \\u043c\\u0430\\u0448\\u0438\\u043d , \\u0447\\u0435\\u0440\\u0435\\u0437 10 \\u043c\\u0438\\u043d\\u0443\\u0442 \\u0431\\u044b\\u043b \\u0432 \\u043f\\u0440\\u0438\\u0435\\u043c\\u043d \\u043e\\u0442\\u0434\\u0435\\u043b\\u0435\\u043d .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14155,\n        \"samples\": [\n          \"\\u043a\\u043e\\u0433\\u0434\\u0430 \\u0448\\u0430\\u0440\\u0430\\u0445\\u043d\\u0443\\u0442\\u044c \\u043e\\u043d\\u0438 6 \\u0432 \\u0433\\u043e\\u0434 , \\u0438\\u0437 \\u043e\\u043d\\u0438 4 \\u0437\\u0430 \\u043c\\u0435\\u0441\\u044f\\u0446-\\u0432\\u043e\\u043e\\u0431\\u0449\\u0435 \\u0441\\u0442\\u0430\\u0442\\u044c \\u043e\\u0431\\u044b\\u0447\\u043d\\u044b\\u0439 \\u0434\\u0435\\u043b\\u043e \\u0442\\u043e\\u0436\\u0435 )\",\n          \"\\u0433\\u043e\\u0434 7 \\u043d\\u0430\\u0437\\u0430\\u0434 \\u0436\\u0438\\u0442\\u044c \\u0432 \\u0434\\u043e\\u043c \\u0441 \\u043e\\u0434\\u0438\\u043d \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434 . \\u0437\\u0432\\u043e\\u043d\\u0438\\u0442\\u044c \\u0432 \\u0441\\u043a\\u043e\\u0440\\u044b\\u0439 \\u043f\\u043e \\u0442\\u0440\\u0430\\u0432\\u043c\\u0430 \\u0433\\u043e\\u043b\\u043e\\u0432\\u0430 \\u0443 \\u0431\\u043b\\u0438\\u0437\\u043a\\u0438\\u0439 \\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a \\u0438 \\u043e\\u043f\\u0435\\u0440\\u0430\\u0442\\u043e\\u0440 \\u043e\\u043f\\u0440\\u043e\\u0441\\u0438\\u0442\\u044c \\u043e \\u0432\\u0435\\u0441\\u044c \\u0434\\u043e\\u0445\\u043e\\u0434\\u0438\\u0442\\u044c \\u0434\\u043e \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441 \\u043d\\u043e\\u043c\\u0435\\u0440 \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434 , \\u0433\\u043e\\u0432\\u043e\\u0440\\u0438\\u0442\\u044c , \\u0447\\u0442\\u043e \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434 \\u0432\\u0435\\u0441\\u044c 1 \\u0438 \\u043d\\u0430\\u0434 \\u043e\\u043d\\u0438 \\u0434\\u0430\\u0436\\u0435 \\u043d\\u043e\\u043c\\u0435\\u0440 \\u043d\\u0438\\u043a\\u043e\\u0433\\u0434\\u0430 \\u043d\\u0435 \\u0432\\u0438\\u0441\\u0435\\u0442\\u044c . \\u0445\\u043e\\u043b\\u043e\\u0434\\u043d\\u044b\\u0439 \\u0442\\u043e\\u043d \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440\\u0438\\u0442\\u044c \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441 . \\u044f \\u0431\\u044b\\u0441\\u0442\\u0440\\u043e \\u043f\\u043e\\u043f\\u044b\\u0442\\u0430\\u0442\\u044c\\u0441\\u044f \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440\\u0438\\u0442\\u044c \\u0442\\u043e\\u0442 \\u0436\\u0435 \\u043e\\u0442\\u0432\\u0435\\u0442 , \\u0442\\u0430\\u043a \\u043a\\u0430\\u043a \\u0431\\u043e\\u044f\\u0442\\u044c\\u0441\\u044f \\u043e\\u0448\\u0438\\u0431\\u0438\\u0442\\u044c\\u0441\\u044f \\u0441 23 \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434 , \\u0432\\u0434\\u0440\\u0443\\u0433 \\u0436\\u0435 \\u0441\\u043a\\u043e\\u0440\\u044b\\u0439 \\u043a \\u043e\\u043d \\u043f\\u043e\\u0434\\u044a\\u0435\\u0445\\u0430\\u0442\\u044c \\u0438 \\u043e\\u043d\\u0430 \\u0431\\u044b\\u0442\\u044c \\u0434\\u0430\\u043b\\u0435\\u043a\\u043e \\u043f\\u043e\\u0442\\u043e\\u043c \\u0438\\u0434\\u0442\\u0438 \\u0434\\u043e \\u043c\\u043e\\u0439 \\u0431\\u0435\\u0437 \\u043d\\u043e\\u043c\\u0435\\u0440 . 4 \\u043f\\u043e\\u043f\\u044b\\u0442\\u043a\\u0430 \\u0438 \\u044f \\u0431\\u0440\\u043e\\u0441\\u0438\\u0442\\u044c \\u0442\\u0440\\u0443\\u0431\\u043a\\u0430 , \\u0432\\u044b\\u0432\\u0435\\u0441\\u0442\\u0438 \\u0442\\u0440\\u0430\\u0432\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u044b\\u0439 \\u043d\\u0430 \\u0443\\u043b\\u0438\\u0446\\u0430 \\u0438 \\u0441\\u0440\\u0430\\u0437\\u0443 \\u044d\\u0442\\u043e\\u0442 \\u043a\\u0430\\u0440\\u0442\\u0438\\u043d\\u0430 \\u0437\\u0430\\u043c\\u0435\\u0442\\u0438\\u0442\\u044c \\u0441\\u043e\\u0441\\u0435\\u0434 \\u0441 \\u043c\\u0430\\u0448\\u0438\\u043d\\u0430 , \\u0447\\u0435\\u0440\\u0435\\u0437 10 \\u043c\\u0438\\u043d\\u0443\\u0442\\u0430 \\u0431\\u044b\\u0442\\u044c \\u0432 \\u043f\\u0440\\u0438\\u0451\\u043c\\u043d\\u044b\\u0439 \\u043e\\u0442\\u0434\\u0435\\u043b\\u0435\\u043d\\u0438\\u0435 .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUPo6VGCxBi-",
        "outputId": "c6296cd6-0af0-4df2-9904-4cd8aa8c19d7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14412 entries, 0 to 14411\n",
            "Data columns (total 4 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   comment              14412 non-null  object \n",
            " 1   toxic                14412 non-null  float64\n",
            " 2   stemmed_comments     14412 non-null  object \n",
            " 3   lemmatized_comments  14412 non-null  object \n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 450.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vL_6360exV-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pymorphy2\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Загрузка данных из CSV файла\n",
        "df = pd.read_csv('labeled.csv')\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Функция для лемматизации и стемминга\n",
        "def lemmatize_and_stem(text):\n",
        "    words = text.split()\n",
        "    lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "# Применение лемматизации и стемминга к комментариям\n",
        "df['lemmatized_comment'] = df['comment'].apply(lemmatize_and_stem)\n",
        "\n",
        "# Создание матрицы признаков BoW\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(df['lemmatized_comment'])\n",
        "\n",
        "# Вывод результатов\n",
        "print(df[['comment', 'lemmatized_comment']].head())\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDzotnQ2TvVK",
        "outputId": "9040c9d6-96b3-4acd-ad8c-d14a57479db7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comment  \\\n",
            "0               Верблюдов-то за что? Дебилы, бл...\\n   \n",
            "1  Хохлы, это отдушина затюканого россиянина, мол...   \n",
            "2                          Собаке - собачья смерть\\n   \n",
            "3  Страницу обнови, дебил. Это тоже не оскорблени...   \n",
            "4  тебя не убедил 6-страничный пдф в том, что Скр...   \n",
            "\n",
            "                                  lemmatized_comment  \n",
            "0                   верблюд-то за что? дебилы, бл...  \n",
            "1  хохлы, это отдушина затюканый россиянина, мол,...  \n",
            "2                            собака - собачий смерть  \n",
            "3  страница обнови, дебил. это тоже не оскорблени...  \n",
            "4  ты не убедить 6-страничный пдф в том, что скри...  \n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nU8XUKlcTvXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KL9gKNjaTvZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas pymorphy2 sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QOyWgkpTva9",
        "outputId": "d108a13b-9bce-4f8c-e2bf-9945ba3dfbf3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pymorphy2\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Загрузка данных из CSV файла\n",
        "df = pd.read_csv('labeled.csv')\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Функция для лемматизации и стемминга\n",
        "def lemmatize_and_stem(text):\n",
        "    words = text.split()\n",
        "    lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "# Применение лемматизации и стемминга к комментариям\n",
        "df['lemmatized_comments'] = df['comment'].apply(lemmatize_and_stem)\n",
        "\n",
        "# Создание матрицы признаков BoW\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(df['lemmatized_comments'])\n",
        "\n",
        "# Вывод результатов\n",
        "print(df[['comment', 'lemmatized_comments']].head())\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbDEMshITvcl",
        "outputId": "ed7766cf-c1bd-49c9-895e-2352b038c4af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comment  \\\n",
            "0               Верблюдов-то за что? Дебилы, бл...\\n   \n",
            "1  Хохлы, это отдушина затюканого россиянина, мол...   \n",
            "2                          Собаке - собачья смерть\\n   \n",
            "3  Страницу обнови, дебил. Это тоже не оскорблени...   \n",
            "4  тебя не убедил 6-страничный пдф в том, что Скр...   \n",
            "\n",
            "                                 lemmatized_comments  \n",
            "0                   верблюд-то за что? дебилы, бл...  \n",
            "1  хохлы, это отдушина затюканый россиянина, мол,...  \n",
            "2                            собака - собачий смерть  \n",
            "3  страница обнови, дебил. это тоже не оскорблени...  \n",
            "4  ты не убедить 6-страничный пдф в том, что скри...  \n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygMaTip1VEdZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tQmXRmFtVEgB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf_pMZS3VEiK",
        "outputId": "4cf8b090-34fb-426b-f9f9-d9e5b3cb830c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHRAb0VGVEkZ",
        "outputId": "41787a81-9102-467c-8f3e-aef59ef697f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "2SeWOKP4VEml"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных из CSV файла\n",
        "df = pd.read_csv('labeled.csv')\n",
        "\n",
        "# Инициализация стеммера и лемматизатора\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Функция для стемминга\n",
        "def stem_sentences(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize_sentences(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Применение стемминга и лемматизации к комментариям\n",
        "df['stemmed_comments'] = df['comment'].apply(stem_sentences)\n",
        "df['lemmatized_comments'] = df['comment'].apply(lemmatize_sentences)\n",
        "\n",
        "# Создание матрицы признаков BoW для стеммированных комментариев\n",
        "vectorizer = CountVectorizer(stop_words=stopwords.words('russian'))\n",
        "bow_matrix_stemmed = vectorizer.fit_transform(df['stemmed_comments'])\n",
        "\n",
        "# Создание матрицы признаков BoW для лемматизированных комментариев\n",
        "vectorizer = CountVectorizer(stop_words=stopwords.words('russian'))\n",
        "bow_matrix_lemmatized = vectorizer.fit_transform(df['lemmatized_comments'])\n",
        "\n",
        "# Вывод результатов\n",
        "print(df[['comment', 'stemmed_comments', 'lemmatized_comments']].head())\n",
        "print(\"BoW Matrix for Stemmed Comments:\")\n",
        "print(bow_matrix_stemmed.toarray())\n",
        "print(\"BoW Matrix for Lemmatized Comments:\")\n",
        "print(bow_matrix_lemmatized.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZZnBehTVEox",
        "outputId": "0878cfd9-cc7d-415f-f2b7-e3ce539f177f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comment  \\\n",
            "0               Верблюдов-то за что? Дебилы, бл...\\n   \n",
            "1  Хохлы, это отдушина затюканого россиянина, мол...   \n",
            "2                          Собаке - собачья смерть\\n   \n",
            "3  Страницу обнови, дебил. Это тоже не оскорблени...   \n",
            "4  тебя не убедил 6-страничный пдф в том, что Скр...   \n",
            "\n",
            "                                    stemmed_comments  \\\n",
            "0              верблюдов-то за что ? дебилы , бл ...   \n",
            "1  хохлы , это отдушина затюканого россиянина , м...   \n",
            "2                            собаке - собачья смерть   \n",
            "3  страницу обнови , дебил . это тоже не оскорбле...   \n",
            "4  тебя не убедил 6-страничный пдф в том , что ск...   \n",
            "\n",
            "                                 lemmatized_comments  \n",
            "0              Верблюдов-то за что ? Дебилы , бл ...  \n",
            "1  Хохлы , это отдушина затюканого россиянина , м...  \n",
            "2                            Собаке - собачья смерть  \n",
            "3  Страницу обнови , дебил . Это тоже не оскорбле...  \n",
            "4  тебя не убедил 6-страничный пдф в том , что Ск...  \n",
            "BoW Matrix for Stemmed Comments:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "BoW Matrix for Lemmatized Comments:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pey3oJpwVErA",
        "outputId": "f30be12a-3dae-4d1a-a0a7-755f0cf5ff2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['comment', 'toxic', 'stemmed_comments', 'lemmatized_comments'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWSjMJOKaIK-",
        "outputId": "c8ffb86c-54a1-4fe3-cfec-f5512e067a2a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14412 entries, 0 to 14411\n",
            "Data columns (total 4 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   comment              14412 non-null  object \n",
            " 1   toxic                14412 non-null  float64\n",
            " 2   stemmed_comments     14412 non-null  object \n",
            " 3   lemmatized_comments  14412 non-null  object \n",
            "dtypes: float64(1), object(3)\n",
            "memory usage: 450.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MxsDi2ncaINN",
        "outputId": "dcabd873-b98c-4ac8-e911-a4ce1e4787e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             comment  toxic  \\\n",
              "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0   \n",
              "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0   \n",
              "2                          Собаке - собачья смерть\\n    1.0   \n",
              "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0   \n",
              "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0   \n",
              "\n",
              "                                    stemmed_comments  \\\n",
              "0              верблюдов-то за что ? дебилы , бл ...   \n",
              "1  хохлы , это отдушина затюканого россиянина , м...   \n",
              "2                            собаке - собачья смерть   \n",
              "3  страницу обнови , дебил . это тоже не оскорбле...   \n",
              "4  тебя не убедил 6-страничный пдф в том , что ск...   \n",
              "\n",
              "                                 lemmatized_comments  \n",
              "0              Верблюдов-то за что ? Дебилы , бл ...  \n",
              "1  Хохлы , это отдушина затюканого россиянина , м...  \n",
              "2                            Собаке - собачья смерть  \n",
              "3  Страницу обнови , дебил . Это тоже не оскорбле...  \n",
              "4  тебя не убедил 6-страничный пдф в том , что Ск...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f6dc12e5-c6bc-473d-bf8c-44a39145a5cd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>toxic</th>\n",
              "      <th>stemmed_comments</th>\n",
              "      <th>lemmatized_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
              "      <td>1.0</td>\n",
              "      <td>верблюдов-то за что ? дебилы , бл ...</td>\n",
              "      <td>Верблюдов-то за что ? Дебилы , бл ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>хохлы , это отдушина затюканого россиянина , м...</td>\n",
              "      <td>Хохлы , это отдушина затюканого россиянина , м...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Собаке - собачья смерть\\n</td>\n",
              "      <td>1.0</td>\n",
              "      <td>собаке - собачья смерть</td>\n",
              "      <td>Собаке - собачья смерть</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>страницу обнови , дебил . это тоже не оскорбле...</td>\n",
              "      <td>Страницу обнови , дебил . Это тоже не оскорбле...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>тебя не убедил 6-страничный пдф в том , что ск...</td>\n",
              "      <td>тебя не убедил 6-страничный пдф в том , что Ск...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6dc12e5-c6bc-473d-bf8c-44a39145a5cd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f6dc12e5-c6bc-473d-bf8c-44a39145a5cd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f6dc12e5-c6bc-473d-bf8c-44a39145a5cd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-28a2dfa4-173c-4f43-b0c0-e5a43fa7401d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-28a2dfa4-173c-4f43-b0c0-e5a43fa7401d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-28a2dfa4-173c-4f43-b0c0-e5a43fa7401d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 14412,\n  \"fields\": [\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14412,\n        \"samples\": [\n          \"\\u0431\\u0435\\u0437\\u0440\\u043e\\u0434\\u043d\\u044b\\u0439 \\u043f\\u043e\\u0442\\u043e\\u043c\\u043e\\u043a \\u0445\\u043e\\u043b\\u043e\\u043f\\u0430 \\u0440\\u0430\\u0441\\u0441\\u0443\\u0436\\u0434\\u0430\\u0435\\u0442 \\u043e \\u043d\\u0430\\u0446\\u0438\\u043e\\u043d\\u0430\\u043b\\u044c\\u043d\\u043e\\u0439 \\u0433\\u043e\\u0440\\u0434\\u043e\\u0441\\u0442\\u0438.\\n\",\n          \"\\u0418\\u043d\\u0442\\u0435\\u0440\\u0435\\u0441\\u043d\\u0430\\u044f \\u0442\\u0435\\u043c\\u0430, \\u043e\\u0434\\u043d\\u0430\\u043a\\u043e. \\u041e\\u0422\\u041f \\u0432\\u0440\\u043e\\u0434\\u0435 \\u0432\\u0435\\u043d\\u0433\\u0435\\u0440\\u0441\\u043a\\u0438\\u0439 \\u0431\\u0430\\u043d\\u043a, \\u0432 \\u0412\\u0435\\u043d\\u0433\\u0440\\u0438\\u0438 \\u043e\\u043d \\u0441\\u0430\\u043c\\u044b\\u0439 \\u043f\\u043e\\u043f\\u0443\\u043b\\u044f\\u0440\\u043d\\u044b\\u0439, \\u0443 \\u043c\\u0435\\u043d\\u044f \\u0443 \\u0441\\u0430\\u043c\\u043e\\u0433\\u043e \\u0435\\u0433\\u043e \\u0441\\u0447\\u0451\\u0442 \\u0438 \\u043a\\u0430\\u0440\\u0442\\u0430, \\u0438\\u0431\\u043e \\u0443 \\u043d\\u0435\\u0433\\u043e \\u0434\\u043e\\u0433\\u043e\\u0432\\u043e\\u0440 \\u0441 \\u0443\\u043d\\u0438\\u0432\\u0435\\u0440\\u043e\\u043c, \\u0441\\u043a\\u0438\\u0434\\u043a\\u0438-\\u043f\\u043b\\u044e\\u0448\\u043a\\u0438-\\u0432\\u043e\\u0437\\u0432\\u0440\\u0430\\u0442 \\u0438 \\u0442.\\u043f. \\u0434\\u043b\\u044f \\u0441\\u0442\\u0443\\u0434\\u0435\\u043d\\u0442\\u043e\\u0432. \\u0418 \\u0437\\u0430 4 \\u0433\\u043e\\u0434\\u0430 \\u043f\\u043e\\u043b\\u044c\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f \\u043d\\u0438 \\u043e\\u0434\\u043d\\u043e\\u0439 \\u043f\\u0440\\u043e\\u0431\\u043b\\u0435\\u043c\\u044b, \\u043d\\u0438 \\u043e\\u0434\\u043d\\u043e\\u0439 \\u043f\\u043e\\u0434\\u043a\\u043b\\u044e\\u0447\\u0451\\u043d\\u043d\\u043e\\u0439 \\u0443\\u0441\\u043b\\u0443\\u0433\\u0438, \\u0431\\u043b\\u043e\\u043a\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f, \\u0441\\u043f\\u0438\\u0441\\u0430\\u043d\\u0438\\u044f \\u043d\\u0438 \\u0437\\u0430 \\u0447\\u0442\\u043e \\u0438 \\u043f\\u043e\\u0434\\u043e\\u0431\\u043d\\u043e\\u0439 \\u0435\\u0440\\u0435\\u0441\\u0438, \\u043a\\u043e\\u0442\\u043e\\u0440\\u043e\\u0439 \\u0441\\u0442\\u0440\\u0430\\u0434\\u0430\\u044e\\u0442 \\u0432\\u0441\\u0435 \\u0440\\u043e\\u0441\\u0441\\u0438\\u0439\\u0441\\u043a\\u0438\\u0435 \\u0431\\u0430\\u043d\\u043a\\u0438. \\u041d\\u0438\\u043a\\u0430\\u043a\\u0438\\u0445 \\u043a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043d\\u044b\\u0445 \\u043a\\u0430\\u0440\\u0442 \\u0434\\u0430\\u0436\\u0435 \\u043d\\u0435 \\u043f\\u0440\\u0435\\u0434\\u043b\\u0430\\u0433\\u0430\\u044e\\u0442 (\\u0438 \\u0432\\u043e\\u043e\\u0431\\u0449\\u0435 \\u043e \\u043f\\u043e\\u0434\\u043e\\u0431\\u043d\\u044b\\u0445 \\u0438\\u0441\\u0442\\u043e\\u0440\\u0438\\u044f\\u0445 \\u043d\\u0435 \\u0441\\u043b\\u044b\\u0448\\u0430\\u043b). \\u0412\\u044b\\u0445\\u043e\\u0434\\u0438\\u0442, \\u0431\\u0430\\u043d\\u043a \\u0442\\u043e\\u0442 \\u0436\\u0435, \\u0430 \\u043f\\u0440\\u0438\\u043d\\u0446\\u0438\\u043f \\u0440\\u0430\\u0431\\u043e\\u0442\\u044b \\u0434\\u0440\\u0443\\u0433\\u043e\\u0439, \\u0437\\u0430\\u0442\\u043e\\u0447\\u0435\\u043d\\u043d\\u044b\\u0439 \\u043f\\u043e\\u0434 \\u0440\\u043e\\u0441\\u0441\\u0438\\u0439\\u0441\\u043a\\u0438\\u0439 \\u043a\\u043b\\u0438\\u0435\\u043d\\u0442 - \\u043b\\u043e\\u0445 . P.S. \\u0412 \\u0412\\u0435\\u043d\\u0433\\u0440\\u0438\\u0438 \\u0432\\u0441\\u0435 \\u0431\\u0430\\u043d\\u043a\\u0438 \\u0448\\u043b\\u044e\\u0442 \\u043a\\u0430\\u0440\\u0442\\u044b \\u043f\\u043e \\u043f\\u043e\\u0447\\u0442\\u0435, \\u043c\\u043e\\u044f \\u043a\\u0430\\u043a \\u0440\\u0430\\u0437 \\u0432 \\u044d\\u0442\\u043e\\u043c \\u043c\\u0435\\u0441\\u044f\\u0446\\u0435 \\u043f\\u0440\\u0438\\u0448\\u043b\\u0430. \\u041d\\u043e \\u0432 \\u043f\\u043e\\u0447\\u0442\\u043e\\u0432\\u044b\\u0439 \\u044f\\u0449\\u0438\\u043a \\u0438\\u0445 \\u043d\\u0438 \\u0437\\u0430 \\u0447\\u0442\\u043e \\u043d\\u0435 \\u043a\\u0438\\u043d\\u0443\\u0442. \\u041b\\u0438\\u0431\\u043e \\u0432 \\u0440\\u0443\\u043a\\u0438, \\u043b\\u0438\\u0431\\u043e \\u0438\\u0437\\u0432\\u0435\\u0449\\u0435\\u043d\\u0438\\u0435 \\u0447\\u0442\\u043e\\u0431 \\u0437\\u0430\\u0431\\u0440\\u0430\\u043b \\u043d\\u0430 \\u043f\\u043e\\u0447\\u0442\\u0435. \\u0410 \\u043f\\u043e\\u0447\\u0442\\u0430 \\u043f\\u043e\\u0440\\u044f\\u0434\\u043e\\u0447\\u043d\\u0430\\u044f\\n\",\n          \"\\u0421\\u0443\\u0442\\u044c \\u0442\\u0440\\u0435\\u0431\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f \\u043f\\u0440\\u043e \\u041230 - \\u043d\\u0435 \\u043f\\u0440\\u043e\\u0447\\u043d\\u043e\\u0441\\u0442\\u044c, \\u0430 \\u0432\\u043e\\u0434\\u043e\\u043d\\u0435\\u043f\\u0440\\u043e\\u043d\\u0438\\u0446\\u0430\\u0435\\u043c\\u043e\\u0441\\u0442\\u044c. \\u0412\\u043e\\u0434\\u043e\\u043d\\u0435\\u043f\\u0440\\u043e\\u043d\\u0438\\u0446\\u0430\\u0435\\u043c\\u043e\\u0441\\u0442\\u044c \\u0432\\u043e\\u043e\\u0431\\u0449\\u0435 \\u0442\\u043e \\u043e\\u0431\\u043e\\u0437\\u043d\\u0430\\u0447\\u0430\\u0435\\u0442\\u0441\\u044f W, \\u043d\\u0443 \\u0434\\u0430 \\u043d\\u0435 \\u0441\\u0443\\u0442\\u044c, \\u0437\\u0430\\u0447\\u0435\\u043c \\u043c\\u043d\\u0435 \\u043e\\u043d\\u0430 \\u0432 \\u043b\\u0435\\u043d\\u0442\\u0435 \\u043f\\u043e\\u0434 \\u0437\\u0435\\u043c\\u043b\\u0451\\u0439 ?\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"toxic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.47195781877088117,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stemmed_comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14155,\n        \"samples\": [\n          \"\\u043a\\u043e\\u0433\\u0434\\u0430 \\u0448\\u0430\\u0440\\u0430\\u0445\\u043d\\u0443\\u043b \\u0438\\u0445 6 \\u0432 \\u0433\\u043e\\u0434\\u0443 , \\u0438\\u0437 \\u043d\\u0438\\u0445 4 \\u0437\\u0430 \\u043c\\u0435\\u0441\\u044f\\u0446-\\u0432\\u043e\\u043e\\u0431\\u0449\\u0435 \\u0441\\u0442\\u0430\\u043b\\u043e \\u043e\\u0431\\u044b\\u0447\\u043d\\u044b\\u043c \\u0434\\u0435\\u043b\\u043e\\u043c \\u0442\\u043e\\u0436\\u0435 )\",\n          \"\\u043b\\u0435\\u0442 7 \\u043d\\u0430\\u0437\\u0430\\u0434 \\u0436\\u0438\\u043b \\u0432 \\u0434\\u043e\\u043c\\u0435 \\u0441 \\u043e\\u0434\\u043d\\u0438\\u043c \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434\\u043e\\u043c . \\u0437\\u0432\\u043e\\u043d\\u0438\\u043b \\u0432 \\u0441\\u043a\\u043e\\u0440\\u0443\\u044e \\u043f\\u043e \\u0442\\u0440\\u0430\\u0432\\u043c\\u0435 \\u0433\\u043e\\u043b\\u043e\\u0432\\u044b \\u0443 \\u0431\\u043b\\u0438\\u0437\\u043a\\u043e\\u0433\\u043e \\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\\u0430 \\u0438 \\u043e\\u043f\\u0435\\u0440\\u0430\\u0442\\u043e\\u0440 \\u043e\\u043f\\u0440\\u043e\\u0441\\u0438\\u0432 \\u043e\\u0431\\u043e \\u0432\\u0441\\u0435\\u043c \\u0434\\u043e\\u0445\\u043e\\u0434\\u0438\\u0442 \\u0434\\u043e \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441\\u0430 \\u043d\\u043e\\u043c\\u0435\\u0440 \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434\\u0430 , \\u0433\\u043e\\u0432\\u043e\\u0440\\u044e , \\u0447\\u0442\\u043e \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434 \\u0432\\u0441\\u0435\\u0433\\u043e 1 \\u0438 \\u043d\\u0430\\u0434 \\u043d\\u0438\\u043c \\u0434\\u0430\\u0436\\u0435 \\u043d\\u043e\\u043c\\u0435\\u0440\\u0430 \\u043d\\u0438\\u043a\\u043e\\u0433\\u0434\\u0430 \\u043d\\u0435 \\u0432\\u0438\\u0441\\u0435\\u043b\\u043e . \\u0445\\u043e\\u043b\\u043e\\u0434\\u043d\\u044b\\u043c \\u0442\\u043e\\u043d\\u043e\\u043c \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440\\u0438\\u043b\\u0430 \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441 . \\u044f \\u0431\\u044b\\u0441\\u0442\\u0440\\u043e \\u043f\\u043e\\u043f\\u044b\\u0442\\u0430\\u043b\\u0441\\u044f \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440\\u0438\\u0442\\u044c \\u0442\\u043e\\u0442 \\u0436\\u0435 \\u043e\\u0442\\u0432\\u0435\\u0442 , \\u0442\\u0430\\u043a \\u043a\\u0430\\u043a \\u0431\\u043e\\u044f\\u043b\\u0441\\u044f \\u043e\\u0448\\u0438\\u0431\\u0438\\u0442\\u044c\\u0441\\u044f \\u0441 23 \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434\\u043e\\u043c , \\u0432\\u0434\\u0440\\u0443\\u0433 \\u0436\\u0435 \\u0441\\u043a\\u043e\\u0440\\u0430\\u044f \\u043a \\u043d\\u0435\\u043c\\u0443 \\u043f\\u043e\\u0434\\u044a\\u0435\\u0434\\u0435\\u0442 \\u0438 \\u0435\\u0439 \\u0431\\u0443\\u0434\\u0435\\u0442 \\u0434\\u0430\\u043b\\u0435\\u043a\\u043e \\u043f\\u043e\\u0442\\u043e\\u043c \\u0438\\u0434\\u0442\\u0438 \\u0434\\u043e \\u043c\\u043e\\u0435\\u0433\\u043e \\u0431\\u0435\\u0437 \\u043d\\u043e\\u043c\\u0435\\u0440\\u0430 . 4 \\u043f\\u043e\\u043f\\u044b\\u0442\\u043a\\u0438 \\u0438 \\u044f \\u0431\\u0440\\u043e\\u0441\\u0438\\u043b \\u0442\\u0440\\u0443\\u0431\\u043a\\u0443 , \\u0432\\u044b\\u0432\\u0435\\u043b \\u0442\\u0440\\u0430\\u0432\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u043e\\u0433\\u043e \\u043d\\u0430 \\u0443\\u043b\\u0438\\u0446\\u0443 \\u0438 \\u0441\\u0440\\u0430\\u0437\\u0443 \\u044d\\u0442\\u0443 \\u043a\\u0430\\u0440\\u0442\\u0438\\u043d\\u0443 \\u0437\\u0430\\u043c\\u0435\\u0442\\u0438\\u043b \\u0441\\u043e\\u0441\\u0435\\u0434 \\u0441 \\u043c\\u0430\\u0448\\u0438\\u043d\\u043e\\u0439 , \\u0447\\u0435\\u0440\\u0435\\u0437 10 \\u043c\\u0438\\u043d\\u0443\\u0442 \\u0431\\u044b\\u043b\\u0438 \\u0432 \\u043f\\u0440\\u0438\\u0451\\u043c\\u043d\\u043e\\u043c \\u043e\\u0442\\u0434\\u0435\\u043b\\u0435\\u043d\\u0438\\u0438 .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14155,\n        \"samples\": [\n          \"\\u041a\\u043e\\u0433\\u0434\\u0430 \\u0448\\u0430\\u0440\\u0430\\u0445\\u043d\\u0443\\u043b \\u0438\\u0445 6 \\u0432 \\u0433\\u043e\\u0434\\u0443 , \\u0438\\u0437 \\u043d\\u0438\\u0445 4 \\u0437\\u0430 \\u043c\\u0435\\u0441\\u044f\\u0446-\\u0432\\u043e\\u043e\\u0431\\u0449\\u0435 \\u0441\\u0442\\u0430\\u043b\\u043e \\u043e\\u0431\\u044b\\u0447\\u043d\\u044b\\u043c \\u0434\\u0435\\u043b\\u043e\\u043c \\u0442\\u043e\\u0436\\u0435 )\",\n          \"\\u041b\\u0435\\u0442 7 \\u043d\\u0430\\u0437\\u0430\\u0434 \\u0436\\u0438\\u043b \\u0432 \\u0434\\u043e\\u043c\\u0435 \\u0441 \\u043e\\u0434\\u043d\\u0438\\u043c \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434\\u043e\\u043c . \\u0417\\u0432\\u043e\\u043d\\u0438\\u043b \\u0432 \\u0421\\u043a\\u043e\\u0440\\u0443\\u044e \\u043f\\u043e \\u0442\\u0440\\u0430\\u0432\\u043c\\u0435 \\u0433\\u043e\\u043b\\u043e\\u0432\\u044b \\u0443 \\u0431\\u043b\\u0438\\u0437\\u043a\\u043e\\u0433\\u043e \\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\\u0430 \\u0438 \\u043e\\u043f\\u0435\\u0440\\u0430\\u0442\\u043e\\u0440 \\u043e\\u043f\\u0440\\u043e\\u0441\\u0438\\u0432 \\u043e\\u0431\\u043e \\u0432\\u0441\\u0435\\u043c \\u0434\\u043e\\u0445\\u043e\\u0434\\u0438\\u0442 \\u0434\\u043e \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441\\u0430 \\u043d\\u043e\\u043c\\u0435\\u0440 \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434\\u0430 , \\u0433\\u043e\\u0432\\u043e\\u0440\\u044e , \\u0447\\u0442\\u043e \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434 \\u0432\\u0441\\u0435\\u0433\\u043e 1 \\u0438 \\u043d\\u0430\\u0434 \\u043d\\u0438\\u043c \\u0434\\u0430\\u0436\\u0435 \\u043d\\u043e\\u043c\\u0435\\u0440\\u0430 \\u043d\\u0438\\u043a\\u043e\\u0433\\u0434\\u0430 \\u043d\\u0435 \\u0432\\u0438\\u0441\\u0435\\u043b\\u043e . \\u0425\\u043e\\u043b\\u043e\\u0434\\u043d\\u044b\\u043c \\u0442\\u043e\\u043d\\u043e\\u043c \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440\\u0438\\u043b\\u0430 \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441 . \\u042f \\u0431\\u044b\\u0441\\u0442\\u0440\\u043e \\u043f\\u043e\\u043f\\u044b\\u0442\\u0430\\u043b\\u0441\\u044f \\u043f\\u043e\\u0432\\u0442\\u043e\\u0440\\u0438\\u0442\\u044c \\u0442\\u043e\\u0442 \\u0436\\u0435 \\u043e\\u0442\\u0432\\u0435\\u0442 , \\u0442\\u0430\\u043a \\u043a\\u0430\\u043a \\u0431\\u043e\\u044f\\u043b\\u0441\\u044f \\u043e\\u0448\\u0438\\u0431\\u0438\\u0442\\u044c\\u0441\\u044f \\u0441 23 \\u043f\\u043e\\u0434\\u044a\\u0435\\u0437\\u0434\\u043e\\u043c , \\u0432\\u0434\\u0440\\u0443\\u0433 \\u0436\\u0435 \\u0441\\u043a\\u043e\\u0440\\u0430\\u044f \\u043a \\u043d\\u0435\\u043c\\u0443 \\u043f\\u043e\\u0434\\u044a\\u0435\\u0434\\u0435\\u0442 \\u0438 \\u0435\\u0439 \\u0431\\u0443\\u0434\\u0435\\u0442 \\u0434\\u0430\\u043b\\u0435\\u043a\\u043e \\u043f\\u043e\\u0442\\u043e\\u043c \\u0438\\u0434\\u0442\\u0438 \\u0434\\u043e \\u043c\\u043e\\u0435\\u0433\\u043e \\u0431\\u0435\\u0437 \\u043d\\u043e\\u043c\\u0435\\u0440\\u0430 . 4 \\u043f\\u043e\\u043f\\u044b\\u0442\\u043a\\u0438 \\u0438 \\u044f \\u0431\\u0440\\u043e\\u0441\\u0438\\u043b \\u0442\\u0440\\u0443\\u0431\\u043a\\u0443 , \\u0432\\u044b\\u0432\\u0435\\u043b \\u0442\\u0440\\u0430\\u0432\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u043e\\u0433\\u043e \\u043d\\u0430 \\u0443\\u043b\\u0438\\u0446\\u0443 \\u0438 \\u0441\\u0440\\u0430\\u0437\\u0443 \\u044d\\u0442\\u0443 \\u043a\\u0430\\u0440\\u0442\\u0438\\u043d\\u0443 \\u0437\\u0430\\u043c\\u0435\\u0442\\u0438\\u043b \\u0441\\u043e\\u0441\\u0435\\u0434 \\u0441 \\u043c\\u0430\\u0448\\u0438\\u043d\\u043e\\u0439 , \\u0447\\u0435\\u0440\\u0435\\u0437 10 \\u043c\\u0438\\u043d\\u0443\\u0442 \\u0431\\u044b\\u043b\\u0438 \\u0432 \\u043f\\u0440\\u0438\\u0451\\u043c\\u043d\\u043e\\u043c \\u043e\\u0442\\u0434\\u0435\\u043b\\u0435\\u043d\\u0438\\u0438 .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SepsuqhpaIQH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Y5cwiDObDQB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Для реализации stemming, lemmatization и BoW на датасете из файла labeled.csv, следует выполнить следующие шаги:"
      ],
      "metadata": {
        "id": "yUaqUj3MbDYl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузить датасет из файла labeled.csv.\n",
        "\n",
        "import pandas as pd\n",
        "# Загрузить датасет\n",
        "data = pd.read_csv('labeled.csv')"
      ],
      "metadata": {
        "id": "v9zS1RR6bDZO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Провести предобработку текста, включая удаление пунктуации, приведение к нижнему регистру и т.д."
      ],
      "metadata": {
        "id": "pVMBqiHtbDZy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
        "    return text\n",
        "\n",
        "data['comment_clean'] = data['comment'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "m865uRUqbDaW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применить stemming и lemmatization к тексту."
      ],
      "metadata": {
        "id": "DlmmcrlqbDj_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_text(text):\n",
        "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "data['stemmed_comment'] = data['comment_clean'].apply(stem_text)\n",
        "data['lemmatized_comment'] = data['comment_clean'].apply(lemmatize_text)\n",
        "\n",
        "print(data['stemmed_comment'])\n",
        "print('\\n')\n",
        "print(data['lemmatized_comment'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOlzhSv9bNPb",
        "outputId": "85d02c6d-1739-4a8d-eac7-37e74d794cf9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                             верблюдовто за что дебилы бл\n",
            "1        хохлы это отдушина затюканого россиянина мол в...\n",
            "2                                    собаке собачья смерть\n",
            "3        страницу обнови дебил это тоже не оскорбление ...\n",
            "4        тебя не убедил пдф в том что скрипалей отравил...\n",
            "                               ...                        \n",
            "14407    вонючий совковый скот прибежал и ноет а вот и ...\n",
            "14408    а кого любить гоблина тупорылого чтоли или как...\n",
            "14409    посмотрел утомленных солнцем и оказалось что э...\n",
            "14410    крымотред нарушает правила раздела тк в нем не...\n",
            "14411    до сих пор пересматриваю его видео орамбо кста...\n",
            "Name: stemmed_comment, Length: 14412, dtype: object\n",
            "\n",
            "\n",
            "0                             верблюдовто за что дебилы бл\n",
            "1        хохлы это отдушина затюканого россиянина мол в...\n",
            "2                                    собаке собачья смерть\n",
            "3        страницу обнови дебил это тоже не оскорбление ...\n",
            "4        тебя не убедил пдф в том что скрипалей отравил...\n",
            "                               ...                        \n",
            "14407    вонючий совковый скот прибежал и ноет а вот и ...\n",
            "14408    а кого любить гоблина тупорылого чтоли или как...\n",
            "14409    посмотрел утомленных солнцем и оказалось что э...\n",
            "14410    крымотред нарушает правила раздела тк в нем не...\n",
            "14411    до сих пор пересматриваю его видео орамбо кста...\n",
            "Name: lemmatized_comment, Length: 14412, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Применить Bag of Words (BoW) к тексту."
      ],
      "metadata": {
        "id": "jS30_829bNRg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vectorizer = CountVectorizer()\n",
        "bow_matrix = bow_vectorizer.fit_transform(data['lemmatized_comment'])\n",
        "\n",
        "# Создать DataFrame для BoW\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
        "print(bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5Iq4Yg4bNTe",
        "outputId": "3f549853-8cf9-4cb2-bc39-e418bf20ab39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       abaqus  ac  academy  accaмблеи  access  account  accountability  \\\n",
            "0           0   0        0          0       0        0               0   \n",
            "1           0   0        0          0       0        0               0   \n",
            "2           0   0        0          0       0        0               0   \n",
            "3           0   0        0          0       0        0               0   \n",
            "4           0   0        0          0       0        0               0   \n",
            "...       ...  ..      ...        ...     ...      ...             ...   \n",
            "14407       0   0        0          0       0        0               0   \n",
            "14408       0   0        0          0       0        0               0   \n",
            "14409       0   0        0          0       0        0               0   \n",
            "14410       0   0        0          0       0        0               0   \n",
            "14411       0   0        0          0       0        0               0   \n",
            "\n",
            "       accwizexe  acronis  activision  ...  ёлка  ёлку  ёмаё  \\\n",
            "0              0        0           0  ...     0     0     0   \n",
            "1              0        0           0  ...     0     0     0   \n",
            "2              0        0           0  ...     0     0     0   \n",
            "3              0        0           0  ...     0     0     0   \n",
            "4              0        0           0  ...     0     0     0   \n",
            "...          ...      ...         ...  ...   ...   ...   ...   \n",
            "14407          0        0           0  ...     0     0     0   \n",
            "14408          0        0           0  ...     0     0     0   \n",
            "14409          0        0           0  ...     0     0     0   \n",
            "14410          0        0           0  ...     0     0     0   \n",
            "14411          0        0           0  ...     0     0     0   \n",
            "\n",
            "       ёмкостейнефтегазоблочного  ёмкости  ёмкость  ёмкостью  ёпта  ёта  \\\n",
            "0                              0        0        0         0     0    0   \n",
            "1                              0        0        0         0     0    0   \n",
            "2                              0        0        0         0     0    0   \n",
            "3                              0        0        0         0     0    0   \n",
            "4                              0        0        0         0     0    0   \n",
            "...                          ...      ...      ...       ...   ...  ...   \n",
            "14407                          0        0        0         0     0    0   \n",
            "14408                          0        0        0         0     0    0   \n",
            "14409                          0        0        0         0     0    0   \n",
            "14410                          0        0        0         0     0    0   \n",
            "14411                          0        0        0         0     0    0   \n",
            "\n",
            "       ётавских  \n",
            "0             0  \n",
            "1             0  \n",
            "2             0  \n",
            "3             0  \n",
            "4             0  \n",
            "...         ...  \n",
            "14407         0  \n",
            "14408         0  \n",
            "14409         0  \n",
            "14410         0  \n",
            "14411         0  \n",
            "\n",
            "[14412 rows x 68455 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Теперь у вас есть предобработанные тексты, примененные stemming, lemmatization и BoW на датасете из файла labeled.csv.\n",
        "# Вы можете использовать эти данные для обучения модели машинного обучения для задачи классификации токсичных комментариев."
      ],
      "metadata": {
        "id": "FcQF8g0UbNVd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NGaN-cUCbNXi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install pandas nltk scikit-learn"
      ],
      "metadata": {
        "id": "WwuUHx4AbNZZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas pymorphy2 pymorphy2-dicts-ru scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWQnmYgZhOlg",
        "outputId": "08a433bc-a63c-4bd3-94db-f5ad8c0bec1d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru in /usr/local/lib/python3.10/dist-packages (2.4.417127.4579844)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Загрузка датасета\n",
        "df = pd.read_csv('labeled.csv')\n",
        "\n",
        "# Инициализация стеммера и лемматизатора\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Функция для проведения стемминга и лемматизации\n",
        "def preprocess_text(text):\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "\n",
        "    # Токенизация текста\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Удаление стоп-слов\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "\n",
        "    # Стемминг и лемматизация\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Объединение токенов обратно в строку\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Применение предобработки к столбцу 'comment'\n",
        "df['comment_processed'] = df['comment'].apply(preprocess_text)\n",
        "\n",
        "# Преобразование текста в Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df['comment_processed'])\n",
        "\n",
        "# Вывод результата\n",
        "print(\"Размерность матрицы BoW:\", X.shape)\n",
        "print(\"Пример Bag of Words для первого комментария:\")\n",
        "print(X[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B6Pr2hxgrZq",
        "outputId": "5d9a0970-a305-4122-f5dd-5c0ac2de1f77"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размерность матрицы BoW: (14412, 68343)\n",
            "Пример Bag of Words для первого комментария:\n",
            "  (0, 7395)\t1\n",
            "  (0, 59807)\t1\n",
            "  (0, 17127)\t1\n",
            "  (0, 66237)\t1\n",
            "  (0, 13446)\t1\n",
            "  (0, 5455)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pymorphy2\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Загрузка датасета\n",
        "df = pd.read_csv('labeled.csv')\n",
        "\n",
        "# Инициализация pymorphy2 для лемматизации\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Функция для проведения лемматизации русского текста\n",
        "def preprocess_text(text):\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "\n",
        "    # Токенизация текста\n",
        "    words = text.split()\n",
        "\n",
        "    # Лемматизация\n",
        "    words = [morph.parse(word)[0].normal_form for word in words]\n",
        "\n",
        "    # Объединение лемм обратно в строку\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Применение предобработки к столбцу 'comment'\n",
        "df['comment_processed'] = df['comment'].apply(preprocess_text)\n",
        "\n",
        "# Преобразование текста в Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df['comment_processed'])\n",
        "\n",
        "# Вывод результата\n",
        "print(\"Размерность матрицы BoW:\", X.shape)\n",
        "print(\"Пример Bag of Words для первого комментария:\")\n",
        "print(X[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_GQXzuhgrbt",
        "outputId": "24635bbc-4c45-40e0-f411-952cd3bbfff9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размерность матрицы BoW: (14412, 50879)\n",
            "Пример Bag of Words для первого комментария:\n",
            "  (0, 6345)\t1\n",
            "  (0, 44433)\t1\n",
            "  (0, 13509)\t1\n",
            "  (0, 49249)\t1\n",
            "  (0, 10743)\t1\n",
            "  (0, 4886)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_uKqnAdGgrdn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4l95gBFgrfk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hwl2ogsugrhV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OFX84nUggrjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}