{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJxaWLrUsUe0d9YB4ldOXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geriys/VKirillV/blob/main/Untitled4_%D0%94%D0%97_3_%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8_%D0%BE%D0%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky3JaSFen9Ns"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.MNIST('.', download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLlPFdK6pyry",
        "outputId": "48f56810-45fd-4254-a442-dac083b2224e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 115261086.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 83254772.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 30631649.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2647745.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучить AE собственной архитектуры на MNIST\n",
        "# Определяем архитектуру автоэнкодера. Будем использовать простую архитектуру с одним скрытым слоем."
      ],
      "metadata": {
        "id": "Gxm4goaRpyuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Кодировщик\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 12),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(12, 3))\n",
        "        # Декодировщик\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(3, 12),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(12, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 28*28),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "fC6NRVKUpyxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Теперь подготовим данные и обучим модель."
      ],
      "metadata": {
        "id": "OrPnnt3lpy0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразования для изображений\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Загрузка данных\n",
        "train_data = datasets.MNIST('.', download=True, train=True, transform=transform)\n",
        "test_data = datasets.MNIST('.', download=True, train=False, transform=transform)\n",
        "\n",
        "# Создание загрузчиков данных\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Создание модели\n",
        "model = Autoencoder()\n",
        "\n",
        "# Оптимизатор и функция потерь\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Обучение модели\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for img, _ in train_loader:\n",
        "        img = img.view(img.size(0), -1)  # Преобразование изображений в одномерный тензор\n",
        "        # Прямой проход\n",
        "        output = model(img)\n",
        "        loss = criterion(output, img)\n",
        "        # Обратное распространение\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1fyyWk-py22",
        "outputId": "5a8c0aed-689d-4b16-c3df-e61d2efe92b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.04902561381459236\n",
            "Epoch 2/10, Loss: 0.03710475191473961\n",
            "Epoch 3/10, Loss: 0.038841601461172104\n",
            "Epoch 4/10, Loss: 0.0355517715215683\n",
            "Epoch 5/10, Loss: 0.03395047038793564\n",
            "Epoch 6/10, Loss: 0.03567918762564659\n",
            "Epoch 7/10, Loss: 0.032188717275857925\n",
            "Epoch 8/10, Loss: 0.039997126907110214\n",
            "Epoch 9/10, Loss: 0.034073300659656525\n",
            "Epoch 10/10, Loss: 0.03812645748257637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создается автоэнкодер, подготавливает данные MNIST, обучает модель и выводит потери на каждой эпохе.\n",
        "# Мы используем функцию потерь MSE (среднеквадратичная ошибка)."
      ],
      "metadata": {
        "id": "wXzF9I7spy5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_70i7idwgZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучить VAE собственной архитектуры на MNIST"
      ],
      "metadata": {
        "id": "3QiVB0o3py_m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создадим собственную архитектуру вариационного автоэнкодера (VAE) и обучим его на наборе данных MNIST.\n",
        "# Будем использовать PyTorch для этого.\n",
        "\n",
        "# Вариационный автоэнкодер состоит из двух частей: энкодера, который создает распределение латентного пространства,\n",
        "# и декодера, который генерирует данные из латентного пространства.\n",
        "# В дополнение к этому, добавим дополнительные слои, которые будут генерировать параметры распределения латентного пространства."
      ],
      "metadata": {
        "id": "vPjdRaL2pzI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "o91XOvhZ1SRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Архитектура VAE\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        # Энкодер\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28*28, 400),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        # Функция для получения среднего и логарифма дисперсии\n",
        "        self.fc_mu = nn.Linear(400, 20)\n",
        "        self.fc_logvar = nn.Linear(400, 20)\n",
        "        # Декодер\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(20, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 28*28),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "\n",
        "# Преобразования для изображений\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Загрузка данных\n",
        "train_data = datasets.MNIST('.', download=True, train=True, transform=transform)\n",
        "test_data = datasets.MNIST('.', download=True, train=False, transform=transform)\n",
        "\n",
        "# Создание загрузчиков данных\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Создание модели\n",
        "model = VAE()\n",
        "\n",
        "# Оптимизатор и функция потерь\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Обучение модели\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.view(-1, 28*28)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset)}')\n",
        "\n",
        "# Функция потерь для VAE\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Обучение модели\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3jfga940iN0",
        "outputId": "591b0104-9bda-464d-d313-fea5b13d1738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====> Epoch: 1 Average loss: 147.9062392578125\n",
            "====> Epoch: 2 Average loss: 115.85168130289713\n",
            "====> Epoch: 3 Average loss: 111.47041239827475\n",
            "====> Epoch: 4 Average loss: 109.41547799886068\n",
            "====> Epoch: 5 Average loss: 108.23716416829427\n",
            "====> Epoch: 6 Average loss: 107.39232587076823\n",
            "====> Epoch: 7 Average loss: 106.81896978352864\n",
            "====> Epoch: 8 Average loss: 106.31922821451823\n",
            "====> Epoch: 9 Average loss: 105.88382544759115\n",
            "====> Epoch: 10 Average loss: 105.59168620605469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXwwg-QC0iXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# *Обучить VAE c переносом стиля на MNIST (на вход декодеру\n",
        "# подавать hidden с таргетным значением, чтобы можно было\n",
        "# нарисовать заданную цифру в заданном стиле)"
      ],
      "metadata": {
        "id": "UTZs-KsOpzNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Для обучения VAE с переносом стиля на MNIST, нужно добавить дополнительный вход в модель,\n",
        "# который будет представлять стиль, который хотим перенести. Это может быть просто одно число,\n",
        "# которое будет использоваться для управления генерацией определенной цифры."
      ],
      "metadata": {
        "id": "bQir7mpd4YXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Количество стилей, которые мы хотим переносить\n",
        "num_styles = 10\n",
        "\n",
        "# Архитектура VAE с переносом стиля\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, num_styles=10):\n",
        "        super(VAE, self).__init__()\n",
        "        # Энкодер\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28*28 + num_styles, 400),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        # Функция для получения среднего и логарифма дисперсии\n",
        "        self.fc_mu = nn.Linear(400, 20)\n",
        "        self.fc_logvar = nn.Linear(400, 20)\n",
        "        # Декодер\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(20 + num_styles, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 28*28),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def forward(self, x, style):\n",
        "        # Создаем входной вектор, который включает стиль\n",
        "        style_onehot = torch.zeros(style.size(0), num_styles).scatter_(1, style.unsqueeze(1), 1)\n",
        "        x = torch.cat((x.view(-1, 28*28), style_onehot), dim=1)\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        # Создаем входной вектор для декодера, который включает стиль\n",
        "        z = torch.cat((z, style_onehot), dim=1)\n",
        "        return self.decoder(z), mu, logvar\n",
        "\n",
        "# Преобразования для изображений\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Загрузка данных\n",
        "train_data = datasets.MNIST('.', download=True, train=True, transform=transform)\n",
        "test_data = datasets.MNIST('.', download=True, train=False, transform=transform)\n",
        "\n",
        "# Создание загрузчиков данных\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Создание модели\n",
        "model = VAE()\n",
        "\n",
        "# Оптимизатор и функция потерь\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Обучение модели\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, style) in enumerate(train_loader):\n",
        "        data = data.view(-1, 28*28)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data, style)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset)}')\n",
        "\n",
        "# Функция потерь для VAE\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Обучение модели\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI45ARx74YZh",
        "outputId": "dc49aa37-0b30-4ae9-88b8-7831d4a1fa50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====> Epoch: 1 Average loss: 145.4082301920573\n",
            "====> Epoch: 2 Average loss: 113.31059157714844\n",
            "====> Epoch: 3 Average loss: 108.52008848876953\n",
            "====> Epoch: 4 Average loss: 106.201405078125\n",
            "====> Epoch: 5 Average loss: 104.78857929280599\n",
            "====> Epoch: 6 Average loss: 103.71729217936198\n",
            "====> Epoch: 7 Average loss: 102.93408013509115\n",
            "====> Epoch: 8 Average loss: 102.29808415527344\n",
            "====> Epoch: 9 Average loss: 101.8237281656901\n",
            "====> Epoch: 10 Average loss: 101.42711516113282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# В этом коде добавили дополнительный вход в модель VAE, который представляет стиль.\n",
        "# Этот вход является one-hot вектором, который создается на основе целевого стиля.\n",
        "# Затем этот вектор добавляется к выходу энкодера и декодера."
      ],
      "metadata": {
        "id": "n0Zq5fmD4Yb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDMczAyH4Yd5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}