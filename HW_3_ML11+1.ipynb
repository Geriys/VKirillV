{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geriys/VKirillV/blob/main/HW_3_ML11%2B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pq8naiX4TkhU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LinearRegression # Линейная регресси\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "_CRcFqIJBuAS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "AXnjSDVEBuGe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "yLOytWY8MtJk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import LassoCV"
      ],
      "metadata": {
        "id": "oty4Frz7BuKZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "tRlEdoB9Bymg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures"
      ],
      "metadata": {
        "id": "LEmlmNQFByrI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "WbSEk4ZnBytK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "eAiM23tIByww"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rQGYH6bTTkhU"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42\n",
        "\n",
        "results_regression = pd.DataFrame(columns = ['model', 'task', 'R2'])\n",
        "results_classification = pd.DataFrame(columns = ['model', 'task', 'f1', 'accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URu-6PsDTkhV"
      },
      "source": [
        "https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "fXAFQQp7TkhV",
        "outputId": "70251e55-c26e-4a0c-928e-4c1e18636d17"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'boston.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d10e5d514777>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'boston.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'boston.csv'"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('boston.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RWFJ4YwTkhW"
      },
      "source": [
        "1. Разделите выборку на обучающую и тестовую в отношении 80%/20%, предварительно выделив целевую переменную (колонка 'MEDV')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x55laT9rTkhW"
      },
      "outputs": [],
      "source": [
        "### Ваш код ###\n",
        "\n",
        "y = data[\"MEDV\"]\n",
        "X = data.drop(columns=[\"MEDV\"])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 10, shuffle = True)\n",
        "#print(f\"Классы в y_train:\\n{y_train}\\n\")\n",
        "#print(f\"Классы в y_test:\\n{y_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0e1QiYeTkhW"
      },
      "source": [
        "2. Обучите стандартную регрессию, а также Ridge и  Lasso с параметрами по умолчанию и выведите их R2 на тестовой выборке"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CITk_7UWTkhW",
        "outputId": "ae4b13c9-acf6-4a02-967f-91d27c6ef113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   model   task        R2\n",
            "0     LR  task2  0.670642\n",
            "1  Ridge  task2  0.665291\n",
            "2  Lasso  task2  0.534676\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "### Ваш код ###\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "# lr_y_pred = lr_model.predict(X_test)\n",
        "\n",
        "ridge_model = Ridge()\n",
        "ridge_model.fit(X_train, y_train)\n",
        "# ridge_y_pred = ridge_model.predict(X_test)\n",
        "\n",
        "lasso_model = Lasso()\n",
        "lasso_model.fit(X_train, y_train)\n",
        "# lasso_y_pred = lasso_model.predict(X_test)\n",
        "\n",
        "# https://scikit-learn.ru/3-1-cross-validation-evaluating-estimator-performance/\n",
        "# https://datafinder.ru/products/ocenka-r2-v-mashinnom-obuchenii#:~:text=%D0%9E%D1%86%D0%B5%D0%BD%D0%BA%D0%B0%20R2%20%E2%80%93%20%D0%BE%D1%87%D0%B5%D0%BD%D1%8C%20%D0%B2%D0%B0%D0%B6%D0%BD%D1%8B%D0%B9%20%D0%BF%D0%BE%D0%BA%D0%B0%D0%B7%D0%B0%D1%82%D0%B5%D0%BB%D1%8C,%D0%B2%20%D0%BF%D1%80%D0%BE%D0%B3%D0%BD%D0%BE%D0%B7%D0%B0%D1%85%2C%20%D0%BE%D0%B1%D1%8A%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%BD%D1%8B%D1%85%20%D0%BD%D0%B0%D0%B1%D0%BE%D1%80%D0%BE%D0%BC%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.\n",
        "# Calculation of R2 Score\n",
        "r2_lr = lr_model.score(X_test, y_test) #cross_val_score(lr_model, X_test, y_test, cv=KFold(n_splits=3), scoring=\"r2\").mean()\n",
        "r2_ridge = ridge_model.score(X_test, y_test) #cross_val_score(ridge_model, X_test, y_test, cv=KFold(n_splits=3), scoring=\"r2\").mean()\n",
        "r2_lasso = lasso_model.score(X_test, y_test) #cross_val_score(lasso_model, X_test, y_test, cv=KFold(n_splits=3), scoring=\"r2\").mean()\n",
        "\n",
        "results_regression.loc[0] = ['LR', 'task2', r2_lr]\n",
        "results_regression.loc[1] = ['Ridge', 'task2', r2_ridge]\n",
        "results_regression.loc[2] = ['Lasso', 'task2', r2_lasso]\n",
        "print(results_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QOXSzOdTkhW"
      },
      "source": [
        "3. Для Ridge и Lasso подберите коэффициент регуляризации двумя способами 1) GridSearchCV, 2) RidgeCV и LassoCV, в пределах от $10^{-5}$ до $10^5$ (по степеням 10). Посчитайте R2 на тестовой выборке по всем моделям и сравните с предыдущими результатами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu1NhjmjTkhW",
        "outputId": "d0f4b105-3df1-4e41-8391-5138c44f47ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['alpha', 'copy_X', 'fit_intercept', 'max_iter', 'positive', 'random_state', 'solver', 'tol'])\n",
            "                model   task        R2\n",
            "0                  LR  task2  0.670642\n",
            "1               Ridge  task2  0.665291\n",
            "2               Lasso  task2  0.534676\n",
            "3  Ridge_GridSearchCV  task3  0.670002\n",
            "4             RidgeCV  task3  0.670002\n",
            "5  Lasso_GridSearchCV  task3  0.670360\n",
            "6             LassoCV  task3  0.670360\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# https://medium.com/nuances-of-programming/%D0%BB%D0%B0%D1%81%D1%81%D0%BE-%D0%B8-%D1%80%D0%B8%D0%B4%D0%B6-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%B8-%D0%B8%D0%BD%D1%82%D1%83%D0%B8%D1%82%D0%B8%D0%B2%D0%BD%D0%BE%D0%B5-%D1%81%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5-a542dd761f62\n",
        "### Ваш код ###\n",
        "\n",
        "\n",
        "# Defining grid of candidate alpha values (powers of 10, from 0.00001 to 1000000)\n",
        "param_grid = {\"alpha\": 10.0 ** np.arange(-5, 6)}\n",
        "\n",
        "# Initializing Ridge and GridSearchCV estimators\n",
        "ridge = Ridge()\n",
        "print(ridge.get_params().keys())\n",
        "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid)\n",
        "# Fitting grid search object\n",
        "grid_search.fit(X_train, y_train)\n",
        "r2_ridge_grid_search = grid_search.score(X_test, y_test)\n",
        "\n",
        "ridge_reg_cv = RidgeCV(alphas=param_grid['alpha'], cv=5)\n",
        "ridge_reg_cv.fit(X_train, y_train)\n",
        "r2_ridge_cv = ridge_reg_cv.score(X_test, y_test)\n",
        "\n",
        "# Initializing Lasso and GridSearchCV estimators\n",
        "lasso = Lasso()\n",
        "grid_search_lasso = GridSearchCV(estimator=lasso, param_grid=param_grid)\n",
        "# Fitting grid search object\n",
        "grid_search_lasso.fit(X_train, y_train)\n",
        "r2_lasso_grid_search = grid_search_lasso.score(X_test, y_test)\n",
        "\n",
        "lasso_reg_cv = LassoCV(alphas=param_grid['alpha'], cv=5)\n",
        "lasso_reg_cv.fit(X_train, y_train)\n",
        "\n",
        "r2_lasso_cv = lasso_reg_cv.score(X_test, y_test)\n",
        "\n",
        "results_regression.loc[3] = ['Ridge_GridSearchCV', 'task3', r2_ridge_grid_search]\n",
        "results_regression.loc[4] = ['RidgeCV', 'task3', r2_ridge_cv]\n",
        "results_regression.loc[5] = ['Lasso_GridSearchCV', 'task3', r2_lasso_grid_search]\n",
        "results_regression.loc[6] = ['LassoCV', 'task3', r2_lasso_cv]\n",
        "print(results_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX1fykfnTkhX"
      },
      "source": [
        "4. Проведите масштабирование выборки (используйте Pipeline, StandardScaler, MinMaxScaler), посчитайте R2 для Ridge и Lasso с параметрами по умолчанию и сравните с предыдущими результатами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5YvUdBkTkhX",
        "outputId": "44b6f410-7339-41ff-ca9c-53ad7c5464bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   model   task        R2\n",
            "0                     LR  task2  0.670642\n",
            "1                  Ridge  task2  0.665291\n",
            "2                  Lasso  task2  0.534676\n",
            "3     Ridge_GridSearchCV  task3  0.670002\n",
            "4                RidgeCV  task3  0.670002\n",
            "5     Lasso_GridSearchCV  task3  0.670360\n",
            "6                LassoCV  task3  0.670360\n",
            "7   Ridge_StandardScaler  task4  0.670844\n",
            "8     Ridge_MinMaxScaler  task4  0.666259\n",
            "9   Lasso_StandardScaler  task4  0.608150\n",
            "10    Lasso_MinMaxScaler  task4  0.094319\n"
          ]
        }
      ],
      "source": [
        "### Ваш код ###\n",
        "\n",
        "\n",
        "estimators = [('scaler', StandardScaler()), ('model', Ridge())]\n",
        "pipe = Pipeline(estimators)\n",
        "r2_ridge_standart_scaler = pipe.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "estimators = [('scaler', MinMaxScaler()), ('model', Ridge())]\n",
        "pipe = Pipeline(estimators)\n",
        "r2_ridge_min_max_scaler = pipe.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "estimators = [('scaler', StandardScaler()), ('model', Lasso())]\n",
        "pipe = Pipeline(estimators)\n",
        "r2_lasso_standart_scaler = pipe.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "estimators = [('scaler', MinMaxScaler()), ('model', Lasso())]\n",
        "pipe = Pipeline(estimators)\n",
        "r2_lasso_min_max_scaler = pipe.fit(X_train, y_train).score(X_test, y_test)\n",
        "results_regression.loc[7] = ['Ridge_StandardScaler', 'task4', r2_ridge_standart_scaler]\n",
        "results_regression.loc[8] = ['Ridge_MinMaxScaler', 'task4', r2_ridge_min_max_scaler]\n",
        "results_regression.loc[9] = ['Lasso_StandardScaler', 'task4', r2_lasso_standart_scaler]\n",
        "results_regression.loc[10] = ['Lasso_MinMaxScaler', 'task4', r2_lasso_min_max_scaler]\n",
        "print(results_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iqNt2MdTkhX"
      },
      "source": [
        "5. Подберите коэффициент регуляризации для Ridge и Lasso на масштабированных данных, посчитайте R2 и сравните с предыдущими результатами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjbPtJrzTkhX",
        "outputId": "ff6774d5-3b45-437f-d4d2-b47fef7480c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      model   task        R2\n",
            "0                        LR  task2  0.670642\n",
            "1                     Ridge  task2  0.665291\n",
            "2                     Lasso  task2  0.534676\n",
            "3        Ridge_GridSearchCV  task3  0.670002\n",
            "4                   RidgeCV  task3  0.670002\n",
            "5        Lasso_GridSearchCV  task3  0.670360\n",
            "6                   LassoCV  task3  0.670360\n",
            "7      Ridge_StandardScaler  task4  0.670844\n",
            "8        Ridge_MinMaxScaler  task4  0.666259\n",
            "9      Lasso_StandardScaler  task4  0.608150\n",
            "10       Lasso_MinMaxScaler  task4  0.094319\n",
            "11  Ridge_StandardScaler_CV  task5  0.671640\n",
            "12    Ridge_MinMaxScaler_CV  task5  0.670629\n",
            "13  Lasso_StandardScaler_CV  task5  0.671214\n",
            "14    Lasso_MinMaxScaler_CV  task5  0.671900\n"
          ]
        }
      ],
      "source": [
        "### Ваш код ###\n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('model', Ridge())])\n",
        "params = { 'model__alpha': 10.0 ** np.arange(-5, 6) }\n",
        "grid_pipeline = GridSearchCV(pipe, params)\n",
        "r2_ridge_standart_scaler_cv = grid_pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()), ('model', Ridge())])\n",
        "params = { 'model__alpha': 10.0 ** np.arange(-5, 6) }\n",
        "grid_pipeline = GridSearchCV(pipe, params)\n",
        "r2_ridge_min_max_scaler_cv = grid_pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('model', Lasso())])\n",
        "params = { 'model__alpha': 10.0 ** np.arange(-5, 6) }\n",
        "grid_pipeline = GridSearchCV(pipe, params)\n",
        "r2_lasso_standart_scaler_cv = grid_pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()), ('model', Lasso())])\n",
        "params = { 'model__alpha': 10.0 ** np.arange(-5, 6) }\n",
        "grid_pipeline = GridSearchCV(pipe, params)\n",
        "r2_lasso_min_max_scaler_cv = grid_pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "results_regression.loc[11] = ['Ridge_StandardScaler_CV', 'task5', r2_ridge_standart_scaler_cv]\n",
        "results_regression.loc[12] = ['Ridge_MinMaxScaler_CV', 'task5', r2_ridge_min_max_scaler_cv]\n",
        "results_regression.loc[13] = ['Lasso_StandardScaler_CV', 'task5', r2_lasso_standart_scaler_cv]\n",
        "results_regression.loc[14] = ['Lasso_MinMaxScaler_CV', 'task5', r2_lasso_min_max_scaler_cv]\n",
        "print(results_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXo4pRZ9TkhX"
      },
      "source": [
        "6. Добавьте попарные произведения признаков и их квадраты (используйте PolynomialFeatures) на масштабированных признаках, посчитайте R2 для Ridge и Lasso с параметрами по умолчанию и сравните с предыдущими результатами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOl7wSk-TkhX",
        "outputId": "b51744e8-5097-42fc-96f7-059bd7bd9c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        model   task        R2\n",
            "0                          LR  task2  0.670642\n",
            "1                       Ridge  task2  0.665291\n",
            "2                       Lasso  task2  0.534676\n",
            "3          Ridge_GridSearchCV  task3  0.670002\n",
            "4                     RidgeCV  task3  0.670002\n",
            "5          Lasso_GridSearchCV  task3  0.670360\n",
            "6                     LassoCV  task3  0.670360\n",
            "7        Ridge_StandardScaler  task4  0.670844\n",
            "8          Ridge_MinMaxScaler  task4  0.666259\n",
            "9        Lasso_StandardScaler  task4  0.608150\n",
            "10         Lasso_MinMaxScaler  task4  0.094319\n",
            "11    Ridge_StandardScaler_CV  task5  0.671640\n",
            "12      Ridge_MinMaxScaler_CV  task5  0.670629\n",
            "13    Lasso_StandardScaler_CV  task5  0.671214\n",
            "14      Lasso_MinMaxScaler_CV  task5  0.671900\n",
            "15  Ridge_StandardScaler_Poly  task6  0.877763\n",
            "16    Ridge_MinMaxScaler_Poly  task6  0.825928\n",
            "17  Lasso_StandardScaler_Poly  task6  0.748369\n",
            "18    Lasso_MinMaxScaler_Poly  task6  0.089644\n"
          ]
        }
      ],
      "source": [
        "### Ваш код ###\n",
        "\n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures()), ('model', Ridge())])\n",
        "r2_ridge_standart_scaler_poly = pipe.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()), ('poly', PolynomialFeatures()), ('model', Ridge())])\n",
        "r2_ridge_min_max_scaler_poly = pipe.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures()), ('model', Lasso())])\n",
        "r2_lasso_standart_scaler_poly = pipe.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()), ('poly', PolynomialFeatures()), ('model', Lasso())])\n",
        "r2_lasso_min_max_scaler_poly = pipe.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "\n",
        "results_regression.loc[15] = ['Ridge_StandardScaler_Poly', 'task6', r2_ridge_standart_scaler_poly]\n",
        "results_regression.loc[16] = ['Ridge_MinMaxScaler_Poly', 'task6', r2_ridge_min_max_scaler_poly]\n",
        "results_regression.loc[17] = ['Lasso_StandardScaler_Poly', 'task6', r2_lasso_standart_scaler_poly]\n",
        "results_regression.loc[18] = ['Lasso_MinMaxScaler_Poly', 'task6', r2_lasso_min_max_scaler_poly]\n",
        "print(results_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCOOmCwlTkhY"
      },
      "source": [
        "7. Подберите коэффициент регуляризации для Ridge и Lasso на масштабированных данных, добавив PolynomialFeatures, посчитайте R2 и сравните с предыдущими результатами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN2w9_y_TkhY",
        "outputId": "025ee3cf-89b5-4376-9b50-11a159e556d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.495e+02, tolerance: 2.514e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.146e+02, tolerance: 2.574e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.237e+02, tolerance: 2.353e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.350e+02, tolerance: 2.483e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.224e+02, tolerance: 2.507e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.176e+02, tolerance: 2.514e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.085e+02, tolerance: 2.574e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.963e+02, tolerance: 2.353e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.174e+02, tolerance: 2.483e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.036e+02, tolerance: 2.507e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.540e+02, tolerance: 2.514e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.809e+02, tolerance: 2.574e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.445e+02, tolerance: 2.353e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.491e+02, tolerance: 2.483e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.699e+02, tolerance: 2.507e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+01, tolerance: 2.483e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.566e+02, tolerance: 2.514e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.174e+02, tolerance: 2.574e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.928e+02, tolerance: 2.353e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.322e+02, tolerance: 2.483e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.071e+02, tolerance: 2.507e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.728e+02, tolerance: 2.514e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.271e+02, tolerance: 2.574e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.819e+02, tolerance: 2.353e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.307e+02, tolerance: 2.483e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.133e+02, tolerance: 2.507e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.942e+02, tolerance: 2.514e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.024e+02, tolerance: 2.574e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.604e+02, tolerance: 2.353e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.892e+02, tolerance: 2.483e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.958e+02, tolerance: 2.507e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.675e+00, tolerance: 2.514e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.817e+01, tolerance: 2.574e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.101e+00, tolerance: 2.483e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           model   task        R2\n",
            "0                             LR  task2  0.670642\n",
            "1                          Ridge  task2  0.665291\n",
            "2                          Lasso  task2  0.534676\n",
            "3             Ridge_GridSearchCV  task3  0.670002\n",
            "4                        RidgeCV  task3  0.670002\n",
            "5             Lasso_GridSearchCV  task3  0.670360\n",
            "6                        LassoCV  task3  0.670360\n",
            "7           Ridge_StandardScaler  task4  0.670844\n",
            "8             Ridge_MinMaxScaler  task4  0.666259\n",
            "9           Lasso_StandardScaler  task4  0.608150\n",
            "10            Lasso_MinMaxScaler  task4  0.094319\n",
            "11       Ridge_StandardScaler_CV  task5  0.671640\n",
            "12         Ridge_MinMaxScaler_CV  task5  0.670629\n",
            "13       Lasso_StandardScaler_CV  task5  0.671214\n",
            "14         Lasso_MinMaxScaler_CV  task5  0.671900\n",
            "15     Ridge_StandardScaler_Poly  task6  0.877763\n",
            "16       Ridge_MinMaxScaler_Poly  task6  0.825928\n",
            "17     Lasso_StandardScaler_Poly  task6  0.748369\n",
            "18       Lasso_MinMaxScaler_Poly  task6  0.089644\n",
            "19  Ridge_StandardScaler_Poly_CV  task7  0.866597\n",
            "20    Ridge_MinMaxScaler_Poly_CV  task7  0.868587\n",
            "21  Lasso_StandardScaler_Poly_CV  task7  0.852894\n",
            "22    Lasso_MinMaxScaler_Poly_CV  task7  0.840998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.853e+01, tolerance: 3.109e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ],
      "source": [
        "### Ваш код ###\n",
        "\n",
        "params = { 'model__alpha': 10.0 ** np.arange(-5, 6) }\n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures()), ('model', Ridge())])\n",
        "grid_pipeline = GridSearchCV(pipe, params)\n",
        "r2_ridge_standart_scaler_poly_cv = grid_pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()), ('poly', PolynomialFeatures()), ('model', Ridge())])\n",
        "grid_pipeline = GridSearchCV(pipe, params)\n",
        "r2_ridge_min_max_scaler_poly_cv = grid_pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures()), ('model', Lasso())])\n",
        "grid_pipeline = GridSearchCV(pipe, params)\n",
        "r2_lasso_standart_scaler_poly_cv = grid_pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()), ('poly', PolynomialFeatures()), ('model', Lasso())])\n",
        "grid_pipeline = GridSearchCV(pipe, params)\n",
        "r2_lasso_min_max_scaler_poly_cv = grid_pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "\n",
        "results_regression.loc[19] = ['Ridge_StandardScaler_Poly_CV', 'task7', r2_ridge_standart_scaler_poly_cv]\n",
        "results_regression.loc[20] = ['Ridge_MinMaxScaler_Poly_CV', 'task7', r2_ridge_min_max_scaler_poly_cv]\n",
        "results_regression.loc[21] = ['Lasso_StandardScaler_Poly_CV', 'task7', r2_lasso_standart_scaler_poly_cv]\n",
        "results_regression.loc[22] = ['Lasso_MinMaxScaler_Poly_CV', 'task7', r2_lasso_min_max_scaler_poly_cv]\n",
        "print(results_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1-68_aaTkhY"
      },
      "source": [
        "8. Подберите наилучшую модель (используйте Pipeline, GridSearchCV) подбирая тип регуляризации (L1,L2), коэффициент регуляризации, метод масштабирования и степень полинома в PolynomialFeatures. Выведите итоговые параметры и результат R2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RlIsM39TkhY"
      },
      "outputs": [],
      "source": [
        "### Ваш код ###\n",
        "params = { 'model__alpha': 10.0 ** np.arange(-5, 6) }\n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures()), ('model', Lasso())])\n",
        "\n",
        "\n",
        "grid_pipeline = GridSearchCV(pipe, params)\n",
        "r2_lasso_standart_scaler_poly_cv = grid_pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "best_params = {cv: 'GridSearchCV', scaler: 'MinMaxScaler', model: 'Ridge(L2)', best_aplha: grid_pipeline.best_params_, }\n",
        "\n",
        "\n",
        "print('Параметры лучшей модели:\\n', best_params)\n",
        "r2_best_model = 0\n",
        "results_regression.loc[23] = ['Best_Model', 'task8', r2_best_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G4s5wD0TkhY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=3)),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', Lasso(random_state=0))\n",
        "])\n",
        "\n",
        "# Создаем GridSearchCV\n",
        "param_grid = {\n",
        "    'model__alpha': np.logspace(-3, 3, 7),\n",
        "    'poly__degree': [1, 2, 3, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='r2')\n",
        "# Запускаем GridSearchCV\n",
        "grid_search.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "best_params = {{cv: 'GridSearchCV', scaler: 'MinMaxScaler', model: 'Ridge(L2)', best_aplha: grid_pipeline.best_params_, }}\n",
        "print('Параметры лучшей модели:\\n', best_params)\n",
        "r2_best_model = 0\n",
        "results_regression.loc[23] = ['Best_Model', 'task8', r2_best_model]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "XCvFSyk_TkhY",
        "outputId": "b37d2cbb-8d20-4603-e824-aff7cedac5cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           model   task        R2\n",
              "0                             LR  task2  0.670642\n",
              "1                          Ridge  task2  0.665291\n",
              "2                          Lasso  task2  0.534676\n",
              "3             Ridge_GridSearchCV  task3  0.670002\n",
              "4                        RidgeCV  task3  0.670002\n",
              "5             Lasso_GridSearchCV  task3  0.670360\n",
              "6                        LassoCV  task3  0.670360\n",
              "7           Ridge_StandardScaler  task4  0.670844\n",
              "8             Ridge_MinMaxScaler  task4  0.666259\n",
              "9           Lasso_StandardScaler  task4  0.608150\n",
              "10            Lasso_MinMaxScaler  task4  0.094319\n",
              "11       Ridge_StandardScaler_CV  task5  0.671640\n",
              "12         Ridge_MinMaxScaler_CV  task5  0.670629\n",
              "13       Lasso_StandardScaler_CV  task5  0.671214\n",
              "14         Lasso_MinMaxScaler_CV  task5  0.671900\n",
              "15     Ridge_StandardScaler_Poly  task6  0.877763\n",
              "16       Ridge_MinMaxScaler_Poly  task6  0.825928\n",
              "17     Lasso_StandardScaler_Poly  task6  0.748369\n",
              "18       Lasso_MinMaxScaler_Poly  task6  0.089644\n",
              "19  Ridge_StandardScaler_Poly_CV  task7  0.866597\n",
              "20    Ridge_MinMaxScaler_Poly_CV  task7  0.868587\n",
              "21  Lasso_StandardScaler_Poly_CV  task7  0.852894\n",
              "22    Lasso_MinMaxScaler_Poly_CV  task7  0.840998"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e69c8e3-7df2-46b7-904d-f7c7751400ef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>task</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LR</td>\n",
              "      <td>task2</td>\n",
              "      <td>0.670642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ridge</td>\n",
              "      <td>task2</td>\n",
              "      <td>0.665291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lasso</td>\n",
              "      <td>task2</td>\n",
              "      <td>0.534676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ridge_GridSearchCV</td>\n",
              "      <td>task3</td>\n",
              "      <td>0.670002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RidgeCV</td>\n",
              "      <td>task3</td>\n",
              "      <td>0.670002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Lasso_GridSearchCV</td>\n",
              "      <td>task3</td>\n",
              "      <td>0.670360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LassoCV</td>\n",
              "      <td>task3</td>\n",
              "      <td>0.670360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Ridge_StandardScaler</td>\n",
              "      <td>task4</td>\n",
              "      <td>0.670844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Ridge_MinMaxScaler</td>\n",
              "      <td>task4</td>\n",
              "      <td>0.666259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Lasso_StandardScaler</td>\n",
              "      <td>task4</td>\n",
              "      <td>0.608150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Lasso_MinMaxScaler</td>\n",
              "      <td>task4</td>\n",
              "      <td>0.094319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Ridge_StandardScaler_CV</td>\n",
              "      <td>task5</td>\n",
              "      <td>0.671640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Ridge_MinMaxScaler_CV</td>\n",
              "      <td>task5</td>\n",
              "      <td>0.670629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Lasso_StandardScaler_CV</td>\n",
              "      <td>task5</td>\n",
              "      <td>0.671214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Lasso_MinMaxScaler_CV</td>\n",
              "      <td>task5</td>\n",
              "      <td>0.671900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Ridge_StandardScaler_Poly</td>\n",
              "      <td>task6</td>\n",
              "      <td>0.877763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Ridge_MinMaxScaler_Poly</td>\n",
              "      <td>task6</td>\n",
              "      <td>0.825928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Lasso_StandardScaler_Poly</td>\n",
              "      <td>task6</td>\n",
              "      <td>0.748369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Lasso_MinMaxScaler_Poly</td>\n",
              "      <td>task6</td>\n",
              "      <td>0.089644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Ridge_StandardScaler_Poly_CV</td>\n",
              "      <td>task7</td>\n",
              "      <td>0.866597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Ridge_MinMaxScaler_Poly_CV</td>\n",
              "      <td>task7</td>\n",
              "      <td>0.868587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Lasso_StandardScaler_Poly_CV</td>\n",
              "      <td>task7</td>\n",
              "      <td>0.852894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Lasso_MinMaxScaler_Poly_CV</td>\n",
              "      <td>task7</td>\n",
              "      <td>0.840998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e69c8e3-7df2-46b7-904d-f7c7751400ef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0e69c8e3-7df2-46b7-904d-f7c7751400ef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0e69c8e3-7df2-46b7-904d-f7c7751400ef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-baac573e-a840-4e0b-a278-97e432bf6a60\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-baac573e-a840-4e0b-a278-97e432bf6a60')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-baac573e-a840-4e0b-a278-97e432bf6a60 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_regression",
              "summary": "{\n  \"name\": \"results_regression\",\n  \"rows\": 23,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Ridge_StandardScaler_Poly\",\n          \"Lasso_StandardScaler\",\n          \"LR\"\n        ],\n        \"num_unique_values\": 23,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"samples\": [\n          \"task2\",\n          \"task3\",\n          \"task7\"\n        ],\n        \"num_unique_values\": 6,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20266977854653184,\n        \"min\": 0.08964389476527435,\n        \"max\": 0.8777634766787686,\n        \"samples\": [\n          0.6706419566528606,\n          0.8665972283818332,\n          0.7483688916152148\n        ],\n        \"num_unique_values\": 21,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "results_regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsIhH-b6TkhY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8mu0PVuTkhY"
      },
      "source": [
        "http://archive.ics.uci.edu/ml/datasets/Adult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSLR84s3TkhY",
        "outputId": "3efe7374-c3b4-4db7-9e2f-c9d6bbc6f710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<=50K' '>50K']\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('adult.csv')\n",
        "print(data['class'].unique())#.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HQwI4o6TkhY"
      },
      "source": [
        "9. Разделите выборку на признаки и целевую переменную(колонка class). Замените целевую переменную на числовые значения ('<=50K' - 1, '>50K' - 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "nNJmKBpUTkhZ"
      },
      "outputs": [],
      "source": [
        "### Ваш код ###\n",
        "data[\"class-num\"] = np.where(data[\"class\"] == \"<=50K\", 1, 0)\n",
        "y = data[\"class-num\"]\n",
        "X = data.drop(columns=[\"class\", \"class-num\"])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 10, shuffle = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSrLMeSlTkhZ"
      },
      "source": [
        "10. Посчитайте метрики accuracy и f1_score на предсказании только самого частого класса в целевой переменной.\n",
        "\n",
        "> Блок с отступами\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Определить самый часто встречающийся класс 1 или 0.\n",
        "#Облучить модель фитом как обычно // треин тест сплит на отфильтрованой выборке самого частого класса\n",
        "#на том что получилось найти аккураси и ф1 скор\n",
        "\n",
        "print(len(data[data[\"class-num\"] == 1]))\n",
        "print(len(data[data[\"class-num\"] == 0]))\n",
        "# Самый частый класс - 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds7lAYH0Zw1l",
        "outputId": "3b306881-7470-42ff-fc5f-dad8cd93bff7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37155\n",
            "11687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWOEeq9vTkhZ",
        "outputId": "3764dd47-81a6-4df6-8729-a6954698aadc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "### Ваш код ###\n",
        "print(type(X_test))\n",
        "# X_train_most_frequent = X_test[X_test[\"class-num\"] == 1]\n",
        "# y_train_most_frequent = y_test[y_test[\"class-num\"] == 1]\n",
        "\n",
        "f1_most_frequent = 0\n",
        "acc_most_frequent = 0\n",
        "results_classification.loc[0] = ['Most Frequent class', 'task10', f1_most_frequent, acc_most_frequent]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Загрузка данных\n",
        "data = pd.read_csv('adult.csv')\n",
        "\n",
        "# Замена целевой переменной на числовые значения\n",
        "data[\"class-num\"] = np.where(data[\"class\"] == \"<=50K\", 1, 0)\n",
        "\n",
        "# Разделение на целевую переменную и признаки\n",
        "y = data[\"class-num\"]\n",
        "X = data.drop(columns=[\"class\", \"class-num\"])\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 10, shuffle = True)\n",
        "\n",
        "# Определение самого частого класса в целевой переменной\n",
        "most_frequent_class = y_test.mode()[0]\n",
        "\n",
        "# Предсказание самого частого класса для всех объектов в тестовой выборке\n",
        "predictions = np.full(len(y_test), most_frequent_class)\n",
        "\n",
        "# Вычисление метрик\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "cxIHnbtUjsBl",
        "outputId": "4a09836d-0e9e-486e-8488-1483668ea87e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.755758010031733\n",
            "F1 Score: 0.8608908582089553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5J3A6n4ijsLL"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n"
      ],
      "metadata": {
        "id": "oAVfGEkjjsRN"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-bk-r7zljzuB"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMsPN_VUTkhZ"
      },
      "source": [
        "11. Выясните, присутствуют ли в данных пропуски. Если присутствуют, заполните их самыми частыми значениями (используйте SimpleImputer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Загрузка данных\n",
        "data = pd.read_csv('adult.csv', na_values=['?'])\n",
        "\n",
        "# Проверка наличия символа '?'\n",
        "missing_values = data.isin(['?']).sum(axis=0)\n",
        "print(missing_values)\n",
        "\n",
        "# Замена целевой переменной на числовые значения\n",
        "data[\"class-num\"] = np.where(data[\"class\"] == \"<=50K\", 1, 0)\n",
        "\n",
        "# Разделение на целевую переменную и признаки\n",
        "y = data[\"class-num\"]\n",
        "X = data.drop(columns=[\"class\", \"class-num\"])\n",
        "\n",
        "# Проверка наличия пропусков в признаках\n",
        "missing_values_X = X.isnull().sum()\n",
        "print(missing_values_X)\n",
        "\n",
        "# Если есть пропуски, заполните их самыми частыми значениями\n",
        "if missing_values_X.any():\n",
        "    imputer = SimpleImputer(strategy='most_frequent')\n",
        "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=10, shuffle=True)\n",
        "\n",
        "# Определение самого частого класса в целевой переменной\n",
        "most_frequent_class = y_test.mode()[0]\n",
        "\n",
        "# Предсказание самого частого класса для всех объектов в тестовой выборке\n",
        "predictions = np.full(len(y_test), most_frequent_class)\n",
        "\n",
        "# Вычисление метрик\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "4Ygygi1Gjzw-",
        "outputId": "c7da1e6b-2fb4-4839-834b-eca4778636d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age               0\n",
            "workclass         0\n",
            "fnlwgt            0\n",
            "education         0\n",
            "education-num     0\n",
            "marital-status    0\n",
            "occupation        0\n",
            "relationship      0\n",
            "race              0\n",
            "sex               0\n",
            "capital-gain      0\n",
            "capital-loss      0\n",
            "hours-per-week    0\n",
            "native-country    0\n",
            "class             0\n",
            "dtype: int64\n",
            "age                  0\n",
            "workclass         2799\n",
            "fnlwgt               0\n",
            "education            0\n",
            "education-num        0\n",
            "marital-status       0\n",
            "occupation        2809\n",
            "relationship         0\n",
            "race                 0\n",
            "sex                  0\n",
            "capital-gain         0\n",
            "capital-loss         0\n",
            "hours-per-week       0\n",
            "native-country     857\n",
            "dtype: int64\n",
            "Accuracy: 0.755758010031733\n",
            "F1 Score: 0.8608908582089553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "8qR8BH0sTkhZ",
        "outputId": "26f40e7a-25bb-4272-d6ec-46857d033539",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "В данных есть пропуски.\n",
            "Accuracy: 0.755758010031733\n",
            "F1 Score: 0.8608908582089553\n",
            "В данных X_train нет пропусков.\n",
            "В данных X_test нет пропусков.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Проверяем наличие пропусков\n",
        "if (data == '?').any().any():\n",
        "    print(\"В данных есть пропуски.\")\n",
        "\n",
        "    # Инициализируем SimpleImputer для заполнения пропусков самыми частыми значениями\n",
        "    imputer = SimpleImputer(missing_values='?', strategy='most_frequent')\n",
        "\n",
        "    # Обучаем imputer на обучающей выборке\n",
        "    imputer.fit(X_train)\n",
        "\n",
        "    # Заменяем пропуски на самые частые значения в обучающей и тестовой выборках\n",
        "    X_train = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)\n",
        "    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
        "else:\n",
        "    print(\"В данных нет пропусков.\")\n",
        "\n",
        "# Вычисляем метрики accuracy и f1_score для предсказания самого частого класса\n",
        "most_frequent_class = y_train.mode()[0]\n",
        "predictions = [most_frequent_class] * len(y_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "if (X_train == '?').any().any():\n",
        "    print(\"В данных X_train есть пропуски.\")\n",
        "else:\n",
        "    print(\"В данных X_train нет пропусков.\")\n",
        "\n",
        "if (X_test == '?').any().any():\n",
        "    print(\"В данных X_test есть пропуски.\")\n",
        "else:\n",
        "    print(\"В данных X_test нет пропусков.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2pvMePHTkhZ"
      },
      "source": [
        "12. Выберите колонки с числовыми и категориальными переменными (используя возможности pandas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "4kAz71QnTkhZ",
        "outputId": "5111a075-9b06-4c35-a885-e92194247f50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num columns: ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'class-num']\n",
            "category_columns: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'class']\n"
          ]
        }
      ],
      "source": [
        "#tasks+=1\n",
        "\n",
        "### Ваш код ###\n",
        "numeric_columns1 = data.columns[data.dtypes == 'int64'].tolist()\n",
        "category_columns1 = data.columns[data.dtypes == 'object'].tolist()\n",
        "print(f'num columns: {numeric_columns1}')\n",
        "print(f'category_columns: {category_columns1}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Определение типов данных в колонках\n",
        "# data_types = data.dtypes\n",
        "\n",
        "# # Выбор числовых колонок\n",
        "# numeric_columns = data_types[data_types.isin([int, float])].index\n",
        "# print(\"Числовые колонки:\")\n",
        "# print(numeric_columns)\n",
        "\n",
        "# # Выбор категориальных колонок\n",
        "# categorical_columns = data_types[data_types == object].index\n",
        "# print(\"\\nКатегориальные колонки:\")\n",
        "# print(categorical_columns)"
      ],
      "metadata": {
        "id": "swGYr7U-JGZ0",
        "outputId": "2f48abfa-7128-4d11-ac79-f18968dafb88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Числовые колонки:\n",
            "Index([], dtype='object')\n",
            "\n",
            "Категориальные колонки:\n",
            "Index(['workclass', 'education', 'marital-status', 'occupation',\n",
            "       'relationship', 'race', 'sex', 'native-country', 'class'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_8ykv9zlSlf"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EpoOEnz2lSp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmRhmD1LTkhZ"
      },
      "source": [
        "13. Создайте пайплайн по обработке числовых и категориальных значений колонок (используйте OneHotEncoder,MinMaxScaler) и посчитайте cross_val_score по алгоритмам LogisticRegression, KNeighborsClassifier, LinearSVC по метрикам accuracy и f1_score."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "O40-_Q3bdES0"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Предположим, что у Вас есть данные data, где data_num - числовые колонки, а data_cat - категориальные колонки\n",
        "numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "category_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
        "# Создаем пайплайн для числовых и категориальных значений\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_columns.columns),\n",
        "        ('cat', categorical_transformer, category_columns.columns)\n",
        "    ])\n",
        "\n",
        "# Создаем пайплайн с препроцессором и моделью\n",
        "pipelines = {\n",
        "    'Logistic Regression': Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                           ('classifier', LogisticRegression())]),\n",
        "    'KNeighbors Classifier': Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                             ('classifier', KNeighborsClassifier())]),\n",
        "    'Linear SVC': Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                  ('classifier', LinearSVC())])\n",
        "}\n",
        "\n",
        "# Вычисляем cross_val_score для каждой модели по метрикам accuracy и f1_score\n",
        "for name, pipeline in pipelines.items():\n",
        "    scores_accuracy = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
        "    scores_f1 = cross_val_score(pipeline, X, y, cv=5, scoring='f1')\n",
        "\n",
        "    print(f'Model: {name}')\n",
        "    print(f'Accuracy scores: {scores_accuracy}')\n",
        "    print(f'Mean accuracy: {scores_accuracy.mean()}')\n",
        "    print(f'F1 scores: {scores_f1}')\n",
        "    print(f'Mean F1 score: {scores_f1.mean()}')"
      ],
      "metadata": {
        "id": "Y3wGFE7-dEVq",
        "outputId": "fc44254a-c517-476b-d895-7456c5d7febd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'columns'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-d43b5da2b095>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m preprocessor = ColumnTransformer(\n\u001b[1;32m     14\u001b[0m     transformers=[\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m'num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     ])\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'columns'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "K77HRw7HJIZS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "QbcoxBEpaCAq"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Определяем числовые и категориальные колонки\n",
        "numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "category_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Создаем преобразователи для числовых и категориальных колонок\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "category_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Создаем колонный преобразователь\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_columns),\n",
        "        ('cat', category_transformer, category_columns)\n",
        "    ])\n",
        "\n",
        "# Создаем пайплайн с преобразователем и классификатором\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', LogisticRegression())])\n",
        "\n",
        "# Создаем список алгоритмов классификации\n",
        "classifiers = [\n",
        "    LogisticRegression(),\n",
        "    KNeighborsClassifier(),\n",
        "    LinearSVC()\n",
        "]\n",
        "\n",
        "# Создаем список метрик\n",
        "metrics = [\n",
        "    ('accuracy', make_scorer(accuracy_score)),\n",
        "    ('f1_score', make_scorer(f1_score))\n",
        "]\n",
        "\n",
        "# Удаляем колонку 'class-num' из данных перед передачей их в пайплайн\n",
        "X = data.drop(columns=['class-num'])\n",
        "y = data['class-num']\n",
        "\n",
        "# Вычисляем cross_val_score для каждого алгоритма и каждой метрики\n",
        "for clf in classifiers:\n",
        "    pipeline.set_params(classifier=clf)\n",
        "    print(f\"Classifier: {type(clf).__name__}\")\n",
        "    try:\n",
        "        for metric_name, metric_scorer in metrics:\n",
        "            scores = cross_val_score(pipeline, X, y, cv=5, scoring=metric_scorer)\n",
        "            print(f\"{metric_name}: {scores.mean()}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Failed to compute cross-validation score: {e}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "iWK5nStOPT87",
        "outputId": "67e49b33-8519-4712-f8aa-e59467592606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier: LogisticRegression\n",
            "Failed to compute cross-validation score: \n",
            "All the 5 fits failed.\n",
            "It is very likely that your model is misconfigured.\n",
            "You can try to debug the error by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'class-num'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 448, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'class-num'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 401, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 353, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 724, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 426, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 456, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "\n",
            "Classifier: KNeighborsClassifier\n",
            "Failed to compute cross-validation score: \n",
            "All the 5 fits failed.\n",
            "It is very likely that your model is misconfigured.\n",
            "You can try to debug the error by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'class-num'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 448, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'class-num'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 401, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 353, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 724, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 426, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 456, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "\n",
            "Classifier: LinearSVC\n",
            "Failed to compute cross-validation score: \n",
            "All the 5 fits failed.\n",
            "It is very likely that your model is misconfigured.\n",
            "You can try to debug the error by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'class-num'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 448, in _get_column_indices\n",
            "    col_idx = all_columns.get_loc(col)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'class-num'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 401, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 353, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 724, in fit_transform\n",
            "    self._validate_column_callables(X)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 426, in _validate_column_callables\n",
            "    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 456, in _get_column_indices\n",
            "    raise ValueError(\"A given column is not a column of the dataframe\") from e\n",
            "ValueError: A given column is not a column of the dataframe\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Определяем числовые и категориальные колонки\n",
        "numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "category_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Создание пайплайна для числовых значений\n",
        "numeric_transformer = make_pipeline(MinMaxScaler())\n",
        "\n",
        "# Создание пайплайна для категориальных значений\n",
        "categorical_transformer = make_pipeline(OneHotEncoder())\n",
        "\n",
        "# Объединение пайплайнов\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_columns),\n",
        "        ('cat', categorical_transformer, category_columns)\n",
        "    ])\n",
        "\n",
        "# Проверка качества моделей с помощью cross_val_score\n",
        "models = [\n",
        "    LogisticRegression(),\n",
        "    KNeighborsClassifier(),\n",
        "    LinearSVC()\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    full_pipeline = make_pipeline(preprocessor, model)\n",
        "    scores_accuracy = cross_val_score(full_pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    scores_f1 = cross_val_score(full_pipeline, X_train, y_train, cv=5, scoring='f1')\n",
        "\n",
        "    print(f\"Model: {model.__class__.__name__}\")\n",
        "    print(\"Mean accuracy:\", scores_accuracy.mean())\n",
        "    print(\"Mean f1 score:\", scores_f1.mean())"
      ],
      "metadata": {
        "id": "wb8yX34GaCFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yTlFFZl6aCJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Определяем числовые и категориальные колонки\n",
        "numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "category_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Создаем преобразователи для числовых и категориальных колонок\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "category_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Создаем колонный преобразователь\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_columns),\n",
        "        ('cat', category_transformer, category_columns)\n",
        "    ])\n",
        "\n",
        "# Создаем пайплайн с преобразователем и классификатором\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', LogisticRegression())])\n",
        "\n",
        "# Создаем список алгоритмов классификации\n",
        "classifiers = [\n",
        "    LogisticRegression(),\n",
        "    KNeighborsClassifier(),\n",
        "    LinearSVC()\n",
        "]\n",
        "\n",
        "# Создаем список метрик\n",
        "metrics = [\n",
        "    ('accuracy', make_scorer(accuracy_score)),\n",
        "    ('f1_score', make_scorer(f1_score))\n",
        "]\n",
        "\n",
        "# Вычисляем cross_val_score для каждого алгоритма и каждой метрики\n",
        "for clf in classifiers:\n",
        "    pipeline.set_params(classifier=clf)\n",
        "    print(f\"Classifier: {type(clf).__name__}\")\n",
        "    for metric_name, metric_scorer in metrics:\n",
        "        scores = cross_val_score(pipeline, X, y, cv=5, scoring=metric_scorer)\n",
        "        print(f\"{metric_name}: {scores.mean()}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "Xmml1Zq-JGgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Определяем числовые и категориальные колонки\n",
        "numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "category_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Создаем преобразователи для числовых и категориальных колонок\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "category_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Создаем колонный преобразователь\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_columns),\n",
        "        ('cat', category_transformer, category_columns)\n",
        "    ])\n",
        "\n",
        "# Создаем пайплайн с преобразователем и классификатором\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', LogisticRegression())])\n",
        "\n",
        "# Создаем список алгоритмов классификации\n",
        "classifiers = [\n",
        "    LogisticRegression(),\n",
        "    KNeighborsClassifier(),\n",
        "    LinearSVC()\n",
        "]\n",
        "\n",
        "# Создаем список метрик\n",
        "metrics = [\n",
        "    ('accuracy', make_scorer(accuracy_score)),\n",
        "    ('f1_score', make_scorer(f1_score))\n",
        "]\n",
        "\n",
        "# Удаляем колонку 'class-num' из данных перед передачей их в пайплайн\n",
        "X = data.drop(columns=['class-num'])\n",
        "y = data['class-num']\n",
        "\n",
        "# Вычисляем cross_val_score для каждого алгоритма и каждой метрики\n",
        "for clf in classifiers:\n",
        "    pipeline.set_params(classifier=clf)\n",
        "    print(f\"Classifier: {type(clf).__name__}\")\n",
        "    for metric_name, metric_scorer in metrics:\n",
        "        scores = cross_val_score(pipeline, X, y, cv=5, scoring=metric_scorer)\n",
        "        print(f\"{metric_name}: {scores.mean()}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "IC4e9UwPJIW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfdhwGE8JGjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXym_kTCTkhZ",
        "outputId": "e81bb63d-01ee-4830-b949-aa02974d623a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'numeric_columns' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8f3918276687>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Объединение пайплайнов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m preprocessor = ColumnTransformer([\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'numeric'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'categorical'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'numeric_columns' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Загрузка данных\n",
        "df = pd.read_csv('boston.csv')\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "#X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 10, shuffle = True)\n",
        "\n",
        "# Создание пайплайна для обработки данных\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('onehot', OneHotEncoder(sparse=False))\n",
        "])\n",
        "\n",
        "# Объединение пайплайнов\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('numeric', numeric_pipeline, numeric_columns),\n",
        "    ('categorical', categorical_pipeline, categorical_columns)\n",
        "])\n",
        "\n",
        "# Обучение моделей\n",
        "logistic_regression = LogisticRegression()\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "linear_svc = LinearSVC()\n",
        "\n",
        "# Создание списка моделей\n",
        "models = [('Logistic Regression', logistic_regression),\n",
        "          ('K-Nearest Neighbors', knn_classifier),\n",
        "          ('Linear SVC', linear_svc)]\n",
        "\n",
        "# Подсчет cross_val_score для каждой модели\n",
        "scores = []\n",
        "for name, model in models:\n",
        "    model.fit(preprocessor.fit_transform(X_train), y_train)\n",
        "    scores.append(cross_val_score(model, preprocessor.fit_transform(X_train), y_train, cv=5, scoring=['accuracy', 'f1_macro']))\n",
        "\n",
        "# Вывод результатов\n",
        "for name, scores in zip(model_names, scores):\n",
        "    print(name)\n",
        "    for metric, score in zip(['accuracy', 'f1_macro'], scores):\n",
        "        print('\\t', metric, score.mean())\n",
        "\n",
        "### Ваш код ###\n",
        "\n",
        "f1_LR = 0\n",
        "acc_LR = 0\n",
        "f1_KNN = 0\n",
        "acc_KNN = 0\n",
        "f1_SVM = 0\n",
        "acc_SVM = 0\n",
        "results_classification.loc[1] = ['LogisticRegression', 'task13', f1_LR, acc_LR]\n",
        "results_classification.loc[2] = ['KNeighborsClassifier', 'task13', f1_KNN, acc_KNN]\n",
        "results_classification.loc[3] = ['LinearSVC', 'task13', f1_SVM, acc_SVM]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVVOg9UBTkhZ"
      },
      "source": [
        "14. Можно заметить что в данных присутствуют значения '?', замените их самыми частыми значениями, (испольуйте SimpleImputer). Посчитайте cross_val_score по алгоритмам LogisticRegression, KNeighborsClassifier, LinearSVC по метрикам accuracy и f1_score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaMSxTDMTkha",
        "outputId": "ce5b1639-aa75-492b-e54f-02535a72de66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['target'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3bf5fdb0049c>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Разделение данных на обучающую и тестовую выборки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Создание пайплайна для обработки данных\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5397\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5398\u001b[0m         \"\"\"\n\u001b[0;32m-> 5399\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5400\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5401\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4505\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4545\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4546\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4547\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6933\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6934\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6935\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6936\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['target'] not found in axis\""
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "### Ваш код ###\n",
        "\n",
        "f1_LR = 0\n",
        "acc_LR = 0\n",
        "f1_KNN = 0\n",
        "acc_KNN = 0\n",
        "f1_SVM = 0\n",
        "acc_SVM = 0\n",
        "results_classification.loc[4] = ['LogisticRegression_impute', 'task14', f1_LR, acc_LR]\n",
        "results_classification.loc[5] = ['KNeighborsClassifier_impute', 'task14', f1_KNN, acc_KNN]\n",
        "results_classification.loc[6] = ['LinearSVC_impute', 'task14', f1_SVM, acc_SVM]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVgWBC2NTkha"
      },
      "source": [
        "15. Посчитайте cross_val_score по тем же алгоритмам и метрикам, если просто удалить значения '?'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THyvvH1MTkhg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "### Ваш код ###\n",
        "\n",
        "f1_LR_del_missings = 0\n",
        "acc_LR_del_missings = 0\n",
        "f1_KNN_del_missings = 0\n",
        "acc_KNN_del_missings = 0\n",
        "f1_SVM_del_missings = 0\n",
        "acc_SVM_del_missings = 0\n",
        "results_classification.loc[7] = ['LogisticRegression_delete_missings', 'task15', f1_LR_del_missings, acc_LR_del_missings]\n",
        "results_classification.loc[8] = ['KNeighborsClassifier_delete_missings', 'task15', f1_KNN_del_missings, acc_KNN_del_missings]\n",
        "results_classification.loc[9] = ['LinearSVC_delete_missings', 'task15', f1_SVM_del_missings, acc_SVM_del_missings]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLgvbTHCTkhg"
      },
      "source": [
        " 16. Посчитайте cross_val_score для RandomForestClassifier,GradientBoostingClassifier на данных с замененными значениями '?' на самые частые значения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98SNt384Tkhg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "### Ваш код ###\n",
        "\n",
        "f1_RF = 0\n",
        "acc_RF = 0\n",
        "f1_GB = 0\n",
        "acc_GB = 0\n",
        "results_classification.loc[10] = ['RandomForestClassifier', 'task16', f1_RF, acc_RF]\n",
        "results_classification.loc[11] = ['GradientBoostingClassifier', 'task16', f1_GB, acc_GB]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo8El7HyTkhh"
      },
      "source": [
        "17. Подберите наилучшую модель, подбирая методы обработки колонок - масштабирование признаков, кодирование признаков и заполнение пропусков. Параметры алгоритмов оставьте по умолчанию. Выведите итоговые параметры и результат accuracy и f1_score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBWSy98HTkhh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "### Ваш код ###\n",
        "\n",
        "best_params = {}\n",
        "print('Параметры лучшей модели:\\n', best_params)\n",
        "f1_best = 0\n",
        "acc_best = 0\n",
        "results_classification.loc[12] = ['Best_Model', 'task17', f1_best, acc_best]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzlhOuCSTkhh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49nXt97pTkhh"
      },
      "outputs": [],
      "source": [
        "results_classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh47NkgrTkhh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw_seKtITkhh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWwInWcoTkhh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}